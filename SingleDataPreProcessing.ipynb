{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject Data PreProcessing\n",
    "* find subject data\n",
    "* concatenate the inter-session data\n",
    "* concatenate the intra-session data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "* Adapt the cleaning cell to also consider empty data points and mark them as noData in the DataFrame\n",
    "* Write gaze definition cell by counting identical collider appearances while 21 consecutive hits are then defined as a gaze caused by an actual fixation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General configuration\n",
    "import os\n",
    "\n",
    "# data_directory: str\n",
    "#     Path to a directory to store data.\n",
    "data_directory = '.'\n",
    "\n",
    "# install_missing_packages: bool\n",
    "#     A flag indicating if missing packages should be automatically installed\n",
    "install_missing_packages = True\n",
    "\n",
    "# use_conda: bool\n",
    "#     A flag indicating if conda should be used for software installation.\n",
    "#     If False, pip will be used. The default is to use conda if jupyter\n",
    "#     is run in a conda environment.\n",
    "use_conda = 'CONDA_EXE' in os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for missing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "def check_package(package, pip_pkg: str = None, conda_pkg: str = None):\n",
    "    \"\"\"Check if a given package is installed. If missing install\n",
    "    it (if global flag `install_missing_packages` is True) either with\n",
    "    pip or with conda (depending on `use_conda`).\n",
    "    \"\"\"\n",
    "    if importlib.util.find_spec(package) is not None:\n",
    "        return  # ok, package is already installed\n",
    "\n",
    "    if not install_missing_packages:\n",
    "        raise RuntimeError(f\"{package} is not installed!\")\n",
    "\n",
    "    if use_conda:\n",
    "        import conda.cli\n",
    "        conda.cli.main('conda', 'install',  '-y', conda_pkg or package)\n",
    "    else:\n",
    "        import subprocess\n",
    "        import sys            \n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', pip_pkg or package])\n",
    "        \n",
    "# This is to exit cells without error tracebacks (cosmetic purpose)\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the required environment (skip if already done)\n",
    "\n",
    "Running the following cell will create a file graphs.yml that can be used to setup a conda environment containing the required packages. If you already downloaded the file from my GitHub, skip the next cell and create the env directly from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing graphs.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile graphs.yml\n",
    "name: graphs\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.6\n",
    "  - jupyter\n",
    "  - imageio\n",
    "  - imageio-ffmpeg\n",
    "  - matplotlib\n",
    "  - scikit-image\n",
    "  - opencv\n",
    "  - networkx\n",
    "  - pandas\n",
    "  - statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Creation\n",
    "To create the environment, open the terminal, go to the directory where you stored the graphs.yml file (the directory of the notebook) and type\n",
    "conda env create -f graphs.yml\n",
    "After running this command you have to activate the environment (Linux/MacOS: conda activate graphs, Windows: activate graphs) and then reopen the notebook in that environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and directory information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nX\n",
    "import glob\n",
    "import scipy.cluster.vq as clusters\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from pandas.plotting import autocorrelation_plot as AC_plot \n",
    "#from statsmodels.graphics import tsaplots\n",
    "#from statsmodels.tsa.stattools import acf\n",
    "from skimage.filters import gaussian\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OG_DATA_PATH = './'\n",
    "condition_single = False #True for Single, False for dyadic\n",
    "filename_keyword = 'Single' if condition_single else 'Dyadic'\n",
    "DATA_PATH = './Data {}/'.format(filename_keyword)\n",
    "\n",
    "PROCESSED_DATA_PATH = './Results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting all subject IDs from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1023]\n"
     ]
    }
   ],
   "source": [
    "# Getting the Folder without hidden files in ascending order \n",
    "DATA_FOLDER = sorted([f for f in os.listdir(DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "\n",
    "subIDs = []\n",
    "for sub in DATA_FOLDER:\n",
    "    if sub[0].isdigit():\n",
    "        subIDs.append(int(sub[0:4]))\n",
    "    else:\n",
    "        pass\n",
    "subIDs = np.unique(subIDs)\n",
    "print(subIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Pathfinding Experiment Files per Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPT_FILE_ATTRS = ['ParticipantID', 'PathsReversed', 'IsDyadic', 'IsLeader'];\n",
    "#subIDs = [1023] # remove to do for all subIDs\n",
    "\n",
    "\n",
    "def map_dict(a_dictionary):\n",
    "    a_subset = {key: value for key, value in a_dictionary.items() if key in EYE_FILE_ATTRS};\n",
    "    return a_subset;\n",
    "\n",
    "# For each subject in subject ID folder, combine all experiment files\n",
    "# and save as a unified final experiment file in Result folder\n",
    "for sub in subIDs:\n",
    "    # Get all files for subject\n",
    "    SUB_FILES = list(filter(lambda x: str(sub) in x, DATA_FOLDER));\n",
    "    \n",
    "    # Sort files into experiment files and eyetracking files\n",
    "    EYETRACKING_FILES = list(filter(lambda x: 'EyeTracking' in x, SUB_FILES));\n",
    "    EYETRACKING_FILES.sort();\n",
    "    \n",
    "    EXPT_FILES = list(filter(lambda x: filename_keyword + 'PathFinding' in x, SUB_FILES));\n",
    "    EXPT_FILES.sort();\n",
    "    \n",
    "    EXPT_DATA = {};\n",
    "        \n",
    "    # For each experiment file\n",
    "    for expt_file in EXPT_FILES:\n",
    "        \n",
    "        # Read JSON data\n",
    "        with open(DATA_PATH + expt_file) as f:\n",
    "            try:\n",
    "                subject_session = json.loads(f.read())\n",
    "            except:\n",
    "                print(\"\\tJSON file \" + expt_file + \" is not valid!\")\n",
    "                continue;\n",
    "        \n",
    "        # If there are more than one experiment files, combine the data of the paths if the \n",
    "        # other trial and subject information matches\n",
    "        if len(EXPT_DATA) > 0:\n",
    "            if(not all([subject_session[x] == EXPT_DATA[x] for x in EXPT_FILE_ATTRS])):\n",
    "                raise Exception(\"Experiment File Data \" + expt_file + \" Does Not Match!\")\n",
    "            else:\n",
    "                EXPT_DATA['Concatenated'] = True;\n",
    "                EXPT_DATA['TrialData'] += subject_session['TrialData'];\n",
    "        \n",
    "        else:\n",
    "            EXPT_DATA = subject_session;\n",
    "            EXPT_DATA['Concatenated'] = False;\n",
    "        \n",
    "    # Check if correct number of paths present in experiment file\n",
    "    if len(EXPT_DATA['TrialData']) != 10:\n",
    "        raise Exception(\"Incorrent number of paths in file for Subject \" + str(sub));\n",
    "        break;\n",
    "        \n",
    "    # Save the final combined file in results to be accessed by eyetracking collection script\n",
    "    expt_file_name = PROCESSED_DATA_PATH + str(sub) + \"_\"+filename_keyword+\"Pathfinding_Final.json\";\n",
    "    with open(expt_file_name, 'w') as fp:\n",
    "        json.dump(EXPT_DATA, fp);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leader : follower map\n",
    "dyadic_pair_map = {\n",
    "    '1021' : '1022',\n",
    "    '1023' : '1013',\n",
    "    '1005' : '1055',\n",
    "    '1074' : '1069',\n",
    "    '1008' : '1058',\n",
    "    '1054' : '1004',\n",
    "    '1011' : '1017',\n",
    "    '1018' : '1057'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the data\n",
    "* Loop through all subjects\n",
    "* extract the session data\n",
    "* combine the data\n",
    "* save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1023 started - 1/1\n",
      "\tET: 1 normalized\n",
      "\tET: 2 normalized\n",
      "\tET: 3 normalized\n",
      "\tET: 4 normalized\n",
      "\tET: 5 normalized\n",
      "\tET: 6 normalized\n",
      "\tET: 7 normalized\n",
      "\tET: 8 normalized\n",
      "\tET: 9 normalized\n",
      "\tET: 10 normalized\n",
      "\t1023 Dyadic eyetracking data saved\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# --------- Preparation ---------\n",
    "\n",
    "Session_save_bool = False # set to True if you want to save each individual session as csv\n",
    "Exploration_save_bool = True # set to True if you want to save the complete exploration as csv\n",
    "subcount = 0\n",
    "\n",
    "# column name list for dataframe\n",
    "col_names =  ['PathNumber',\n",
    "          'timeStampDataPointStart',\n",
    "          'timeStampDataPointEnd',\n",
    "          'serverTimeStampDataPointStart',\n",
    "          'serverTimeStampDataPointEnd',\n",
    "          'hitObjectColliderName', \n",
    "          'ordinalOfHit',\n",
    "          'BitMask',\n",
    "          'hitPointOnObject.x',\n",
    "          'hitPointOnObject.y',\n",
    "          'hitPointOnObject.z',\n",
    "          'hitObjectColliderBoundsCenter.x',\n",
    "          'hitObjectColliderBoundsCenter.y',\n",
    "          'hitObjectColliderBoundsCenter.z']\n",
    "\n",
    "\n",
    "# TODO: Add server times to nohitdict\n",
    "NoHit_dict = {'hitPointOnObject': {'x': 'NaN',\n",
    "                                   'y': 'NaN',\n",
    "                                   'z': 'NaN'},\n",
    "              'hitObjectColliderName': 'NoHit',\n",
    "              'hitObjectColliderBoundsCenter': {'x': 'NaN',\n",
    "                                                'y': 'NaN',\n",
    "                                                'z': 'NaN'},\n",
    "              'ordinalOfHit': 'NaN',\n",
    "              'Session': 'NaN',\n",
    "              'timeStampDataPointStart': 'NaN',\n",
    "              'timeStampDataPointEnd': 'NaN',\n",
    "              'serverTimeStampDataPointStart' : 'NaN',\n",
    "              'serverTimeStampDataPointEnd' : 'NaN',\n",
    "              'BitMask': 'NaN'\n",
    "              }\n",
    "\n",
    "# Remove machine time stamps if single\n",
    "if condition_single:\n",
    "    col_names.pop(3);\n",
    "    col_names.pop(3);\n",
    "    NoHit_dict.pop('serverTimeStampDataPointStart');\n",
    "    NoHit_dict.pop('serverTimeStampDataPointEnd');\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------- MAIN PART ---------------------------\n",
    "\n",
    "\n",
    "# --------- first layer - subject loop ---------\n",
    "\n",
    "for subject in subIDs:\n",
    "    subcount +=1\n",
    "    print('Subject ' + str(subject) + ' started - ' + str(subcount) + '/' + str(len(subIDs)))\n",
    "    \n",
    "    # Create empty dataframe for later concatenation\n",
    "    complete_exploration_df = pd.DataFrame(columns = col_names)\n",
    "    \n",
    "    # get the data files according to the subject\n",
    "    subject_folder = sorted([f for f in DATA_FOLDER \n",
    "                             if f.startswith(str(subject)+'_EyeTrackingData_')], \n",
    "                            key=str.lower) \n",
    "    \n",
    "    # the following works as long as the data name format is as follows:\n",
    "    # 'subjectID'_Expl_S_'SessionNumber'_ET_'EyeTrackingSessionNumber'_'UnixTimestamp'.json\n",
    "    folder_files = list()\n",
    "       \n",
    "    # loop through the subject folder and save all numbers\n",
    "    for file in subject_folder:\n",
    "        folder_files.append(re.findall(r'\\d+', file))\n",
    "    \n",
    "    if len(folder_files) == 0:\n",
    "        raise Exception(\"No eyetracking files in \" + filename_keyword +\" condition for subject \" + str(sub))\n",
    "    # Extract all SubIDs (only one), (and Timestamps)\n",
    "    SubID, UnixTimestamp1, _ = map(list, zip(*folder_files))\n",
    "    \n",
    "    # Get the experiment JSON file for the subject\n",
    "    sub_expt_data = {}\n",
    "    sub_expt_file_name = PROCESSED_DATA_PATH + str(sub) + \"_\"+filename_keyword+\"Pathfinding_Final.json\";\n",
    "    try:\n",
    "        with open(sub_expt_file_name, 'r') as fp:\n",
    "            sub_expt_data = json.load(fp);\n",
    "    except:\n",
    "        raise Exception(\"Could not read experiment file for subject \" + str(sub));\n",
    "    \n",
    "# --------- second layer - exploration session loop ---------\n",
    "    s = 0;\n",
    "    complete_hitpoints_df = pd.DataFrame(columns = col_names)\n",
    "    previous_path_num = -1.0;\n",
    "    # loop over separate eye tracking sessions\n",
    "    for ET_session in subject_folder:\n",
    "        s+=1\n",
    "        #print(\"\\tSession \" + str(s) + \" started\")\n",
    "\n",
    "            # open the JSON file as dictionary\n",
    "        with open(DATA_PATH + ET_session) as f:\n",
    "            try:\n",
    "                subject_session = json.loads(f.read())\n",
    "            except:\n",
    "                print(\"\\tJSON file \" + ET_session + \" is not valid!\")\n",
    "\n",
    "        hitpoint_list = list() # create hitpoint list\n",
    "        \n",
    "        # check if trial is valid:\n",
    "        trial_valid = subject_session['trials'][0]['trialIsValid']\n",
    "        if not trial_valid:\n",
    "            raise Exception(\"Excluding \" + ET_Session + \" because trial is not valid!\");\n",
    "            continue;\n",
    "        \n",
    "        sub_id = subject_session['trials'][0]['participantId']\n",
    "        if str(sub_id) != str(subject):\n",
    "            raise Exception(\"Excluding \" + ET_Session + \" because subject IDs don't match!\");\n",
    "            continue;\n",
    "        \n",
    "        # start timestamp of the session \n",
    "        start_time = subject_session['trials'][0]['timeTrialMeasurementStarted']\n",
    "        \n",
    "        # get trial id (path number)\n",
    "        path_num = subject_session['trials'][0]['trialId']\n",
    "        \n",
    "        expt_path_start_time = sub_expt_data['TrialData'][path_num]['StartMachineTimeStamp'];\n",
    "        expt_path_end_time = sub_expt_data['TrialData'][path_num]['EndMachineTimeStamp'];\n",
    "        \n",
    "        machine_time_stamp_diff = 0.0;\n",
    "        if not condition_single and path_num != previous_path_num:\n",
    "            expt_path_start_srvtime_mS = sub_expt_data['TrialData'][path_num]['StartServerTimeStamp'];\n",
    "            machine_time_stamp_diff = expt_path_start_time - expt_path_start_srvtime_mS / 1000;\n",
    "            previous_path_num = path_num;\n",
    "\n",
    "        # amount of datapoints \n",
    "        Len_subses = len(subject_session['trials'][0]['dataPoints'])\n",
    "\n",
    "        # for loop appending each data point rayCastHit Data\n",
    "        # afterwards adding the timestamp to the dict \n",
    "        # passing if there is (1) no raycast hit and (2) if there is only one raycast hit\n",
    "        for each in subject_session['trials'][0]['dataPoints']:\n",
    "\n",
    "            # account for noHits \n",
    "            if each['rayCastHitsCombinedEyes'] == []:\n",
    "                hitpoint_list.append(NoHit_dict)\n",
    "            else:\n",
    "                # append data point\n",
    "                hitpoint_list.append(each['rayCastHitsCombinedEyes'][0]) \n",
    "                # add path number, timestamp and bitmask\n",
    "                idx = len(hitpoint_list)-1\n",
    "                hitpoint_list[idx]['PathNumber'] = path_num\n",
    "                \n",
    "                # Exclude the data point if it lies outside of the times for a path\n",
    "                if each['timeStampDataPointStart'] < expt_path_start_time or each['timeStampDataPointStart'] > expt_path_end_time:\n",
    "                    hitpoint_list[idx]['PathNumber'] += 100;\n",
    "#                     print(str(idx) + \" Data Pt. Start: \" + str(each['timeStampDataPointStart']));\n",
    "#                     print(str(idx) + \" Data Pt. End: \" + str(each['timeStampDataPointEnd']));\n",
    "#                     print(\"Path Start: \" + str(expt_path_start_time));\n",
    "#                     print(\"Path End: \" + str(expt_path_end_time));\n",
    "#                     print('\\n')\n",
    "\n",
    "                # Add the server time stamp if dyadic\n",
    "                if not condition_single:\n",
    "                    hitpoint_list[idx]['serverTimeStampDataPointStart'] = (each['timeStampDataPointStart'] - machine_time_stamp_diff) * 1000.0;\n",
    "                    hitpoint_list[idx]['serverTimeStampDataPointEnd'] = (each['timeStampDataPointEnd'] - machine_time_stamp_diff) * 1000.0;\n",
    "                hitpoint_list[idx]['timeStampDataPointStart'] = each['timeStampDataPointStart'] - start_time\n",
    "                hitpoint_list[idx]['timeStampDataPointEnd'] = each['timeStampDataPointEnd'] - start_time\n",
    "                hitpoint_list[idx]['BitMask'] = each['combinedGazeValidityBitmask']\n",
    "\n",
    "\n",
    "            try: \n",
    "                # append data point of second raycast hit if it exists\n",
    "                hitpoint_list.append(each['rayCastHitsCombinedEyes'][1])\n",
    "                # add Path number, timestamp and bitmask\n",
    "                idx = len(hitpoint_list)-1\n",
    "                hitpoint_list[idx]['PathNumber'] = hitpoint_list[idx-1]['PathNumber'];\n",
    "                \n",
    "                # Add the server time stamp if dyadic\n",
    "                if not condition_single:\n",
    "                    hitpoint_list[idx]['serverTimeStampDataPointStart'] = (each['timeStampDataPointStart'] - machine_time_stamp_diff) * 1000.0;\n",
    "                    hitpoint_list[idx]['serverTimeStampDataPointEnd'] = (each['timeStampDataPointEnd'] - machine_time_stamp_diff) * 1000.0;\n",
    "                \n",
    "                hitpoint_list[idx]['timeStampDataPointStart'] = each['timeStampDataPointStart'] - start_time\n",
    "                hitpoint_list[idx]['timeStampDataPointEnd'] = each['timeStampDataPointEnd'] - start_time\n",
    "                hitpoint_list[idx]['BitMask'] = each['combinedGazeValidityBitmask']\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # normalize the hitpoint dictionary to get dataframe\n",
    "        hitpoints_df = pd.json_normalize(hitpoint_list)\n",
    "\n",
    "        print(\"\\tET: \" + str(s) + \" normalized\")\n",
    "\n",
    "        complete_hitpoints_df = complete_hitpoints_df.append(hitpoints_df)\n",
    "\n",
    "\n",
    "    # --------- Saving each Session ---------\n",
    "\n",
    "    # If you want to save each session separately, set 'Session_save_bool' to True\n",
    "    if Session_save_bool == True:\n",
    "        try:\n",
    "            if len(subject_data) > 0:\n",
    "                complete_hitpoints_df.to_csv(PROCESSED_DATA_PATH\n",
    "                                             + str(subject)\n",
    "                                             + \"_CompleteSession\"\n",
    "                                             + \"_\" + filename_keyword\n",
    "                                             + \"_Hitpoints.csv\")\n",
    "                print(\"\\t\"\n",
    "                      + str(subject)\n",
    "                      + \" session \"\n",
    "                      + \"_\" + filename_keyword\n",
    "                      + \" saved \")\n",
    "            else: \n",
    "                print(\"\\t\"\n",
    "                      + str(subject)\n",
    "                      + \" - Session \"\n",
    "                      + \"_\" + filename_keyword\n",
    "                      + \" is empty!\")\n",
    "        except:\n",
    "            print(\"\\tCould not save subject \"\n",
    "                  + str(subject)\n",
    "                  + \" session \"\n",
    "                  + \"_\" + filename_keyword\n",
    "                  + \"!\")\n",
    "\n",
    "\n",
    "\n",
    "    # fill the complete exploration dataframe with the separate session data (combining the sessions)\n",
    "    complete_exploration_df = complete_exploration_df.append(complete_hitpoints_df)\n",
    "        \n",
    "    # --------- Saving the Exploration ---------\n",
    "    \n",
    "    # If you want to save the exploration file, set 'Exploration_save_bool' to True\n",
    "    if Exploration_save_bool == True:\n",
    "        # saving the complete exploration\n",
    "        try:\n",
    "            complete_exploration_df.to_csv(PROCESSED_DATA_PATH + str(subject) + \"_Complete\"+filename_keyword+\"_Hitpoints.csv\")\n",
    "            print(\"\\t\" + str(subject) + \" \" + filename_keyword + \" eyetracking data saved\")\n",
    "        except:\n",
    "            print(\"\\tCould not save subject \" + str(subject) + \" \" + filename_keyword + \" eyetracking data!\")\n",
    "    \n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_df = pd.read_csv(PROCESSED_DATA_PATH+'1023_CompleteDyadic_Hitpoints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PathNumber</th>\n",
       "      <th>timeStampDataPointStart</th>\n",
       "      <th>timeStampDataPointEnd</th>\n",
       "      <th>serverTimeStampDataPointStart</th>\n",
       "      <th>serverTimeStampDataPointEnd</th>\n",
       "      <th>hitObjectColliderName</th>\n",
       "      <th>ordinalOfHit</th>\n",
       "      <th>BitMask</th>\n",
       "      <th>hitPointOnObject.x</th>\n",
       "      <th>hitPointOnObject.y</th>\n",
       "      <th>hitPointOnObject.z</th>\n",
       "      <th>hitObjectColliderBoundsCenter.x</th>\n",
       "      <th>hitObjectColliderBoundsCenter.y</th>\n",
       "      <th>hitObjectColliderBoundsCenter.z</th>\n",
       "      <th>Session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>Female_063_NoHands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-286.073700</td>\n",
       "      <td>1.188752</td>\n",
       "      <td>-161.713196</td>\n",
       "      <td>-285.866089</td>\n",
       "      <td>1.320987</td>\n",
       "      <td>-161.400894</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>pavement_C.002</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-282.921753</td>\n",
       "      <td>0.137312</td>\n",
       "      <td>-157.582443</td>\n",
       "      <td>-217.756897</td>\n",
       "      <td>-2.905127</td>\n",
       "      <td>-288.595245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>Female_063_NoHands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-286.073700</td>\n",
       "      <td>1.188752</td>\n",
       "      <td>-161.713196</td>\n",
       "      <td>-285.866089</td>\n",
       "      <td>1.320987</td>\n",
       "      <td>-161.400894</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>pavement_C.002</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-282.921753</td>\n",
       "      <td>0.137312</td>\n",
       "      <td>-157.582443</td>\n",
       "      <td>-217.756897</td>\n",
       "      <td>-2.905127</td>\n",
       "      <td>-288.595245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013393</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>1.488637e+09</td>\n",
       "      <td>Female_063_NoHands</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-286.080048</td>\n",
       "      <td>1.191041</td>\n",
       "      <td>-161.749207</td>\n",
       "      <td>-285.873413</td>\n",
       "      <td>1.319996</td>\n",
       "      <td>-161.436264</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157472</th>\n",
       "      <td>19347</td>\n",
       "      <td>109.0</td>\n",
       "      <td>95.967685</td>\n",
       "      <td>95.970661</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>pavement_G.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>266.072479</td>\n",
       "      <td>-2.608216</td>\n",
       "      <td>180.609634</td>\n",
       "      <td>273.366241</td>\n",
       "      <td>-2.745627</td>\n",
       "      <td>186.282379</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157473</th>\n",
       "      <td>19348</td>\n",
       "      <td>109.0</td>\n",
       "      <td>95.970661</td>\n",
       "      <td>95.970661</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>pavement_G.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>266.072479</td>\n",
       "      <td>-2.608216</td>\n",
       "      <td>180.609634</td>\n",
       "      <td>273.366241</td>\n",
       "      <td>-2.745627</td>\n",
       "      <td>186.282379</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157474</th>\n",
       "      <td>19349</td>\n",
       "      <td>109.0</td>\n",
       "      <td>95.978597</td>\n",
       "      <td>95.981574</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>pavement_G.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>266.030792</td>\n",
       "      <td>-2.609457</td>\n",
       "      <td>180.711060</td>\n",
       "      <td>273.366241</td>\n",
       "      <td>-2.745627</td>\n",
       "      <td>186.282379</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157475</th>\n",
       "      <td>19350</td>\n",
       "      <td>109.0</td>\n",
       "      <td>95.990006</td>\n",
       "      <td>95.992981</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>pavement_G.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>265.978119</td>\n",
       "      <td>-2.609941</td>\n",
       "      <td>180.776611</td>\n",
       "      <td>273.366241</td>\n",
       "      <td>-2.745627</td>\n",
       "      <td>186.282379</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157476</th>\n",
       "      <td>19351</td>\n",
       "      <td>109.0</td>\n",
       "      <td>95.992981</td>\n",
       "      <td>95.992981</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>1.489627e+09</td>\n",
       "      <td>pavement_G.005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>265.978119</td>\n",
       "      <td>-2.609941</td>\n",
       "      <td>180.776611</td>\n",
       "      <td>273.366241</td>\n",
       "      <td>-2.745627</td>\n",
       "      <td>186.282379</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157477 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  PathNumber  timeStampDataPointStart  \\\n",
       "0                0       100.0                 0.001985   \n",
       "1                1       100.0                 0.001985   \n",
       "2                2       100.0                 0.008432   \n",
       "3                3       100.0                 0.008432   \n",
       "4                4         0.0                 0.013393   \n",
       "...            ...         ...                      ...   \n",
       "157472       19347       109.0                95.967685   \n",
       "157473       19348       109.0                95.970661   \n",
       "157474       19349       109.0                95.978597   \n",
       "157475       19350       109.0                95.990006   \n",
       "157476       19351       109.0                95.992981   \n",
       "\n",
       "        timeStampDataPointEnd  serverTimeStampDataPointStart  \\\n",
       "0                    0.008432                   1.488637e+09   \n",
       "1                    0.008432                   1.488637e+09   \n",
       "2                    0.008432                   1.488637e+09   \n",
       "3                    0.008432                   1.488637e+09   \n",
       "4                    0.016368                   1.488637e+09   \n",
       "...                       ...                            ...   \n",
       "157472              95.970661                   1.489627e+09   \n",
       "157473              95.970661                   1.489627e+09   \n",
       "157474              95.981574                   1.489627e+09   \n",
       "157475              95.992981                   1.489627e+09   \n",
       "157476              95.992981                   1.489627e+09   \n",
       "\n",
       "        serverTimeStampDataPointEnd hitObjectColliderName  ordinalOfHit  \\\n",
       "0                      1.488637e+09    Female_063_NoHands           1.0   \n",
       "1                      1.488637e+09        pavement_C.002           2.0   \n",
       "2                      1.488637e+09    Female_063_NoHands           1.0   \n",
       "3                      1.488637e+09        pavement_C.002           2.0   \n",
       "4                      1.488637e+09    Female_063_NoHands           1.0   \n",
       "...                             ...                   ...           ...   \n",
       "157472                 1.489627e+09        pavement_G.005           1.0   \n",
       "157473                 1.489627e+09        pavement_G.005           1.0   \n",
       "157474                 1.489627e+09        pavement_G.005           1.0   \n",
       "157475                 1.489627e+09        pavement_G.005           1.0   \n",
       "157476                 1.489627e+09        pavement_G.005           1.0   \n",
       "\n",
       "        BitMask  hitPointOnObject.x  hitPointOnObject.y  hitPointOnObject.z  \\\n",
       "0           3.0         -286.073700            1.188752         -161.713196   \n",
       "1           3.0         -282.921753            0.137312         -157.582443   \n",
       "2           3.0         -286.073700            1.188752         -161.713196   \n",
       "3           3.0         -282.921753            0.137312         -157.582443   \n",
       "4           3.0         -286.080048            1.191041         -161.749207   \n",
       "...         ...                 ...                 ...                 ...   \n",
       "157472      3.0          266.072479           -2.608216          180.609634   \n",
       "157473      3.0          266.072479           -2.608216          180.609634   \n",
       "157474      3.0          266.030792           -2.609457          180.711060   \n",
       "157475      3.0          265.978119           -2.609941          180.776611   \n",
       "157476      3.0          265.978119           -2.609941          180.776611   \n",
       "\n",
       "        hitObjectColliderBoundsCenter.x  hitObjectColliderBoundsCenter.y  \\\n",
       "0                           -285.866089                         1.320987   \n",
       "1                           -217.756897                        -2.905127   \n",
       "2                           -285.866089                         1.320987   \n",
       "3                           -217.756897                        -2.905127   \n",
       "4                           -285.873413                         1.319996   \n",
       "...                                 ...                              ...   \n",
       "157472                       273.366241                        -2.745627   \n",
       "157473                       273.366241                        -2.745627   \n",
       "157474                       273.366241                        -2.745627   \n",
       "157475                       273.366241                        -2.745627   \n",
       "157476                       273.366241                        -2.745627   \n",
       "\n",
       "        hitObjectColliderBoundsCenter.z  Session  \n",
       "0                           -161.400894      NaN  \n",
       "1                           -288.595245      NaN  \n",
       "2                           -161.400894      NaN  \n",
       "3                           -288.595245      NaN  \n",
       "4                           -161.436264      NaN  \n",
       "...                                 ...      ...  \n",
       "157472                       186.282379      NaN  \n",
       "157473                       186.282379      NaN  \n",
       "157474                       186.282379      NaN  \n",
       "157475                       186.282379      NaN  \n",
       "157476                       186.282379      NaN  \n",
       "\n",
       "[157477 rows x 16 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data points excluded for 1023: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False    156839\n",
       "True        638\n",
       "Name: PathNumber, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"data points excluded for 1023: \");\n",
    "(Result_df['PathNumber'] > 20).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
