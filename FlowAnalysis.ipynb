{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Analysis Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import glob\n",
    "import scipy.cluster.vq as clusters\n",
    "import scipy.sparse as sparse\n",
    "import warnings\n",
    "#import random_graph\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from scipy.special import binom as nchoosek\n",
    "from pandas.plotting import autocorrelation_plot as AC_plot \n",
    "from statsmodels.graphics import tsaplots\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from skimage.filters import gaussian\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
    "from skimage.transform.pyramids import pyramid_expand as expand\n",
    "from skimage.transform.pyramids import pyramid_reduce as reduce\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.colors import ListedColormap\n",
    "from itertools import combinations\n",
    "\n",
    "import Ressources.TransformHelper as TransformHelper\n",
    "import pickle\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x));\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def save_to_disk(data, filepath):\n",
    "    with open(filepath, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "def load_from_disk(filepath):\n",
    "    with open(filepath, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        return data\n",
    "    \n",
    "def bins_labels(bins, **kwargs):\n",
    "    bin_w = (max(bins) - min(bins)) / (len(bins) - 1)\n",
    "    plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "    \n",
    "\n",
    "\n",
    "# Git Paths\n",
    "OG_DATA_PATH = './'\n",
    "GIT_DATA_PATH = './Data Exploration/'\n",
    "GIT_PROCESSED_DATA_PATH = './Results/'\n",
    "GIT_GRAPH_PATH = './Results/Graphs/'\n",
    "RESSOURCES_PATH = './Ressources/'\n",
    "    \n",
    "\n",
    "# Getting the Folder without hidden files in ascending order \n",
    "GIT_PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(GIT_PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "GIT_GRAPH_FOLDER = sorted([f for f in os.listdir(GIT_GRAPH_PATH) if not f.startswith('.')], key=str.lower)\n",
    "\n",
    "#houselist \n",
    "house_file = RESSOURCES_PATH + 'building_collider_list.csv'\n",
    "try:\n",
    "    houselist = pd.read_csv(house_file)\n",
    "except:\n",
    "    print('HouseList could not be loaded!')\n",
    "    \n",
    "    \n",
    "# External data for mapping \n",
    "transform_infos = load_from_disk(\"./Ressources/map_image_transform_infos.pickle\")\n",
    "transform_matrix = transform_infos[\"perspective_transform_matrix\"]\n",
    "\n",
    "# load the city map image\n",
    "white_bg_img = cv2.imread(\"./ressources/map_white.png\")\n",
    "\n",
    "\n",
    "# Global variables\n",
    "fontsize = 20\n",
    "fontweight = 'bold'\n",
    "labelfontsize = 30\n",
    "figurelabels = ['A','B','C','D']\n",
    "\n",
    "\n",
    "green = [0.40,0.80,0.42]\n",
    "blue = [0.27,0.38,0.99]\n",
    "yellow = [0.96,0.73,0.23]\n",
    "darkblue = [0.18, 0.19, 0.69]\n",
    "lightyellow = [0.9763, 0.9831, 0.0538] \n",
    "grey = [0.75,0.75,0.75]\n",
    "white = [1,1,1]\n",
    "black = [0,0,0]\n",
    "\n",
    "# implement parula color map scheme from matlab \n",
    "cm_data = [[0.2081, 0.1663, 0.5292], [0.2116238095, 0.1897809524, 0.5776761905], \n",
    " [0.212252381, 0.2137714286, 0.6269714286], [0.2081, 0.2386, 0.6770857143], \n",
    " [0.1959047619, 0.2644571429, 0.7279], [0.1707285714, 0.2919380952, \n",
    "  0.779247619], [0.1252714286, 0.3242428571, 0.8302714286], \n",
    " [0.0591333333, 0.3598333333, 0.8683333333], [0.0116952381, 0.3875095238, \n",
    "  0.8819571429], [0.0059571429, 0.4086142857, 0.8828428571], \n",
    " [0.0165142857, 0.4266, 0.8786333333], [0.032852381, 0.4430428571, \n",
    "  0.8719571429], [0.0498142857, 0.4585714286, 0.8640571429], \n",
    " [0.0629333333, 0.4736904762, 0.8554380952], [0.0722666667, 0.4886666667, \n",
    "  0.8467], [0.0779428571, 0.5039857143, 0.8383714286], \n",
    " [0.079347619, 0.5200238095, 0.8311809524], [0.0749428571, 0.5375428571, \n",
    "  0.8262714286], [0.0640571429, 0.5569857143, 0.8239571429], \n",
    " [0.0487714286, 0.5772238095, 0.8228285714], [0.0343428571, 0.5965809524, \n",
    "  0.819852381], [0.0265, 0.6137, 0.8135], [0.0238904762, 0.6286619048, \n",
    "  0.8037619048], [0.0230904762, 0.6417857143, 0.7912666667], \n",
    " [0.0227714286, 0.6534857143, 0.7767571429], [0.0266619048, 0.6641952381, \n",
    "  0.7607190476], [0.0383714286, 0.6742714286, 0.743552381], \n",
    " [0.0589714286, 0.6837571429, 0.7253857143], \n",
    " [0.0843, 0.6928333333, 0.7061666667], [0.1132952381, 0.7015, 0.6858571429], \n",
    " [0.1452714286, 0.7097571429, 0.6646285714], [0.1801333333, 0.7176571429, \n",
    "  0.6424333333], [0.2178285714, 0.7250428571, 0.6192619048], \n",
    " [0.2586428571, 0.7317142857, 0.5954285714], [0.3021714286, 0.7376047619, \n",
    "  0.5711857143], [0.3481666667, 0.7424333333, 0.5472666667], \n",
    " [0.3952571429, 0.7459, 0.5244428571], [0.4420095238, 0.7480809524, \n",
    "  0.5033142857], [0.4871238095, 0.7490619048, 0.4839761905], \n",
    " [0.5300285714, 0.7491142857, 0.4661142857], [0.5708571429, 0.7485190476, \n",
    "  0.4493904762], [0.609852381, 0.7473142857, 0.4336857143], \n",
    " [0.6473, 0.7456, 0.4188], [0.6834190476, 0.7434761905, 0.4044333333], \n",
    " [0.7184095238, 0.7411333333, 0.3904761905], \n",
    " [0.7524857143, 0.7384, 0.3768142857], [0.7858428571, 0.7355666667, \n",
    "  0.3632714286], [0.8185047619, 0.7327333333, 0.3497904762], \n",
    " [0.8506571429, 0.7299, 0.3360285714], [0.8824333333, 0.7274333333, 0.3217], \n",
    " [0.9139333333, 0.7257857143, 0.3062761905], [0.9449571429, 0.7261142857, \n",
    "  0.2886428571], [0.9738952381, 0.7313952381, 0.266647619], \n",
    " [0.9937714286, 0.7454571429, 0.240347619], [0.9990428571, 0.7653142857, \n",
    "  0.2164142857], [0.9955333333, 0.7860571429, 0.196652381], \n",
    " [0.988, 0.8066, 0.1793666667], [0.9788571429, 0.8271428571, 0.1633142857], \n",
    " [0.9697, 0.8481380952, 0.147452381], [0.9625857143, 0.8705142857, 0.1309], \n",
    " [0.9588714286, 0.8949, 0.1132428571], [0.9598238095, 0.9218333333, \n",
    "  0.0948380952], [0.9661, 0.9514428571, 0.0755333333], \n",
    " [0.9763, 0.9831, 0.0538]]\n",
    "\n",
    "parula_map = LinearSegmentedColormap.from_list('parula', cm_data)\n",
    "\n",
    "Flow_all_df = pd.read_csv(GIT_GRAPH_PATH+ \"Flow_df.csv\")\n",
    "centrality_df = pd.read_csv(GIT_GRAPH_PATH+ \"centrality_df.csv\")\n",
    "\n",
    "dests_A = [\"Windmill-TaskBuilding_10_1\", \"TaskBuilding_41\", \"TaskBuilding_39\", \"TaskBuilding_7\", \\\n",
    "          \"Building_186\", \"TaskBuilding_29\", \"TaskBuilding_1\", \"TaskBuilding_9\", \"TaskBuilding_4\", \\\n",
    "          \"TaskBuilding_53\"]\n",
    "dests_B = dests_A.copy()\n",
    "dests_B.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(subject, GIT_GRAPH_PATH):\n",
    "    \n",
    "    GIT_GRAPH_FOLDER = sorted([f for f in os.listdir(GIT_GRAPH_PATH) if not f.startswith('.')], key=str.lower)\n",
    "    \n",
    "    subject_folder = sorted([f for f in GIT_GRAPH_FOLDER \n",
    "                             if f.startswith(str(subject)+'_edgelist')], key=str.lower)\n",
    "\n",
    "    if len(subject_folder) != 0:\n",
    "\n",
    "        # open the JSON file as dictionary\n",
    "        with open(GIT_GRAPH_PATH + subject_folder[0]) as f:\n",
    "            try:\n",
    "                edge_list = pd.read_csv(f)\n",
    "            except:\n",
    "                    print(\"\\tCould not load subject \" + str(subject) + \" edgelist!\")\n",
    "\n",
    "    else:\n",
    "        print('Subject ' + str(subject) + ' has no data file!')\n",
    "\n",
    "    # --------- GRAPH CREATION ---------\n",
    "\n",
    "    # create graph from edgelist\n",
    "    G = nx.Graph()\n",
    "    G = nx.from_pandas_edgelist(edge_list, 'Edge1', 'Edge2')\n",
    "\n",
    "    # Remove the NoHit Node\n",
    "    G.remove_node('NoHouse')\n",
    "\n",
    "    # Remove the NoHit Node\n",
    "    G.remove_node('NoHit')\n",
    "    # Setting the node coordinates of each node of the graph\n",
    "\n",
    "    # node list\n",
    "    nodelist = list(G.nodes)\n",
    "    nodearray = np.array(G.nodes)\n",
    "\n",
    "    # coord dict\n",
    "    node_pos = {}\n",
    "\n",
    "    for node in nodelist:\n",
    "        # assign node coordinates\n",
    "        x = houselist['transformed_collidercenter_x'][houselist.target_collider_name==node].values[0]\n",
    "        y = houselist['transformed_collidercenter_y'][houselist.target_collider_name==node].values[0]\n",
    "        node_pos[node] = (x,y) \n",
    "\n",
    "    # set the graph's node coordinates attribute\n",
    "    nx.set_node_attributes(G, node_pos, 'coord')\n",
    "\n",
    "    # Set flow capacity of every edge to 1\n",
    "    nx.set_edge_attributes(G, 1.0, 'capactiy')\n",
    "\n",
    "    # degree dict and list of the graph\n",
    "    degree_dict = dict(G.degree)\n",
    "    degree_list = list(degree_dict.values())\n",
    "    \n",
    "    return G, nodelist, degree_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subIDs_all = []\n",
    "for sub in GIT_PROCESSED_DATA_FOLDER:\n",
    "    if sub[0].isdigit():\n",
    "        subIDs_all.append(int(sub[0:4]))\n",
    "    else:\n",
    "        pass\n",
    "subIDs_all = np.unique(subIDs_all)\n",
    "print(subIDs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_figures = False # if you want the figures to be closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flat_flow = np.array(Flow_all_df.iloc[:,2:]).flatten()\n",
    "\n",
    "flow_len = len(flat_flow)\n",
    "\n",
    "degree_len = len(np.array(centrality_df.iloc[:-2,1:-2]).flatten())\n",
    "\n",
    "if close_figures == True:\n",
    "    plt.close('all')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "figgy = plt.figure(figsize=(15,5), constrained_layout=False)\n",
    "\n",
    "# create grid for different subplots                    \n",
    "gs = gridspec.GridSpec(ncols=2, nrows=1, \n",
    "                          wspace=0.23)\n",
    "\n",
    "ax1 = figgy.add_subplot(gs[0])\n",
    "\n",
    "plt.hist(flat_flow, ec='k', color=blue)\n",
    "\n",
    "plt.xlabel('Flow', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Probability', fontsize=20, fontweight='bold')\n",
    "plt.xticks([10,20,30])\n",
    "plt.yticks([0, 50000, 100000, 150000, 200000, 250000],\n",
    "           [0,round(50000/flow_len, 2),\n",
    "            round(100000/flow_len, 2),\n",
    "            round(150000/flow_len, 2),\n",
    "            round(200000/flow_len, 2),\n",
    "            round(250000/flow_len, 2)])\n",
    "\n",
    "ax1 = plt.gca()\n",
    "ax1.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "ax1.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = figgy.add_subplot(gs[1])\n",
    "\n",
    "plt.hist(np.array(centrality_df.iloc[:-2,1:-2]).flatten(), ec='k', color=blue)\n",
    "\n",
    "plt.xlabel('Degree', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Probability', fontsize=20, fontweight='bold')\n",
    "\n",
    "plt.xticks([10,20,30,40])\n",
    "plt.yticks([0,500,1000,1500,2000],\n",
    "           [0,round(500/degree_len, 2),round(1000/degree_len, 2),round(1500/degree_len, 2),round(2000/degree_len, 2)])\n",
    "\n",
    "ax2 = plt.gca()\n",
    "ax2.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "ax2.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "\n",
    "# Figure Labels\n",
    "ax1.text(-8,270000,figurelabels[0],fontdict={'fontweight':fontweight,'fontsize':labelfontsize})\n",
    "ax2.text(-8,2320,figurelabels[1],fontdict={'fontweight':fontweight,'fontsize':labelfontsize})\n",
    "\n",
    "\n",
    "plt.savefig(GIT_GRAPH_PATH + \"Degree_vs_Flow.png\",\n",
    "            dpi=200,\n",
    "            format=\"PNG\",\n",
    "            transparent=False,\n",
    "            facecolor='white',\n",
    "            bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Flow Df of all path node combinations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the condition\n",
    "conditions = ['Single', 'SingleC' , 'Dyadic']\n",
    "directions = ['A','B']\n",
    "Flow_df = pd.DataFrame()\n",
    "\n",
    "flow_list = []\n",
    "min_degree_start_target_list = []\n",
    "\n",
    "for condition in conditions:\n",
    "    PROCESSED_DATA_PATH = './Results/' + condition + '/'\n",
    "    PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "\n",
    "\n",
    "    # open the subject info file of the condition\n",
    "    with open(PROCESSED_DATA_PATH + condition + '_Performance_Analysis.csv') as f:\n",
    "        try:\n",
    "            subject_data = pd.read_csv(f)\n",
    "        except:\n",
    "                print(\"\\tCould not load subject info - \" + str(condition) + \"!\")\n",
    "\n",
    "    subIDs = subject_data.SubjectID.values\n",
    "\n",
    "    subcount = 0\n",
    "\n",
    "    Flow_df_A = pd.DataFrame(columns=[\"Start\", \"Target\", \"Condition\", \"Direction\", \"Leader\"])\n",
    "    Flow_df_B = pd.DataFrame(columns=[\"Start\", \"Target\", \"Condition\", \"Direction\", \"Leader\"])\n",
    "\n",
    "    for subject in subIDs:\n",
    "        subcount +=1\n",
    "        print('Subject ' + str(subject) + ' started - ' + str(subcount) + '/' + str(len(subIDs)))\n",
    "\n",
    "        # get the data files according to the subject\n",
    "        subject_folder = sorted([f for f in GIT_GRAPH_FOLDER \n",
    "                                 if f.startswith(str(subject)+'_edgelist')], key=str.lower)\n",
    "\n",
    "        if len(subject_folder) != 0:\n",
    "\n",
    "            # open the JSON file as dictionary\n",
    "            with open(GIT_GRAPH_PATH + subject_folder[0]) as f:\n",
    "                try:\n",
    "                    edge_list = pd.read_csv(f)\n",
    "                except:\n",
    "                        print(\"\\tCould not load subject \" + str(subject) + \" edgelist!\")\n",
    "\n",
    "        else:\n",
    "            print('Subject ' + str(subject) + ' has no data file!')\n",
    "            continue \n",
    "\n",
    "\n",
    "\n",
    "        # --------- GRAPH CREATION ---------\n",
    "\n",
    "        # create graph from edgelist\n",
    "        G = nx.Graph()\n",
    "        G = nx.from_pandas_edgelist(edge_list, 'Edge1', 'Edge2')\n",
    "\n",
    "        # Remove the NoHit Node\n",
    "        G.remove_node('NoHouse')\n",
    "\n",
    "        # Remove the NoHit Node\n",
    "        G.remove_node('NoHit')\n",
    "        # Setting the node coordinates of each node of the graph\n",
    "\n",
    "\n",
    "        # node list\n",
    "        nodelist = list(G.nodes)\n",
    "        nodearray = np.array(G.nodes)\n",
    "\n",
    "\n",
    "        # coord dict\n",
    "        node_pos = {}\n",
    "\n",
    "        for node in nodelist:\n",
    "            # assign node coordinates\n",
    "            x = houselist['transformed_collidercenter_x'][houselist.target_collider_name==node].values[0]\n",
    "            y = houselist['transformed_collidercenter_y'][houselist.target_collider_name==node].values[0]\n",
    "            node_pos[node] = (x,y) \n",
    "\n",
    "        # set the graph's node coordinates attribute\n",
    "        nx.set_node_attributes(G, node_pos, 'coord')\n",
    "\n",
    "        # Set flow capacity of every edge to 1\n",
    "        nx.set_edge_attributes(G, 1.0, 'capactiy')\n",
    "\n",
    "        # degree dict and list of the graph\n",
    "        degree_dict = dict(G.degree)\n",
    "        degree_list = list(degree_dict.values())\n",
    "\n",
    "\n",
    "        # --------- MAX FLOW CALCULATION ---------  \n",
    "\n",
    "        # condition A\n",
    "        if subject_data.Condition[subject_data.SubjectID == subject].values[0] == 'A':\n",
    "\n",
    "            Flow_df_A[\"Start\"] = dests_A[:-1]\n",
    "            Flow_df_A[\"Target\"].loc[0:len(dests_A)] = dests_A[1:]\n",
    "            \n",
    "\n",
    "            subject_flow = []\n",
    "\n",
    "            for flow in range(len(Flow_df_A)):\n",
    "\n",
    "                source = Flow_df_A[\"Start\"][flow]\n",
    "                sink = Flow_df_A[\"Target\"][flow]\n",
    "\n",
    "                flow_value, flow_dict = nx.maximum_flow(G, source, sink, capacity='capactiy')\n",
    "                subject_flow.append(flow_value)\n",
    "\n",
    "            Flow_df_A[subject] = np.transpose(subject_flow)\n",
    "            Flow_df_A[\"Condition\"] = condition\n",
    "            Flow_df_A[\"Direction\"] = \"A\"\n",
    "            \n",
    "            if condition == \"Dyadic\":\n",
    "                Flow_df_A[\"Leader\"] = subject_data[subject_data.SubjectID == subject]['Leader?'].values[0].copy()\n",
    "\n",
    "            source_degree = centrality_df[centrality_df.Subject == str(subject)][source].item()\n",
    "            sink_degree = centrality_df[centrality_df.Subject == str(subject)][sink].item()\n",
    "            \n",
    "            min_degree_start_target_list.append(min(source_degree, sink_degree))\n",
    "            flow_list.append(flow_value)\n",
    "        # condition B\n",
    "        elif subject_data.Condition[subject_data.SubjectID == subject].values[0] == 'B':\n",
    "\n",
    "            Flow_df_B[\"Start\"] = dests_B[:-1]\n",
    "            Flow_df_B[\"Target\"].loc[0:len(dests_B)] = dests_B[1:]    \n",
    "\n",
    "            subject_flow = []\n",
    "\n",
    "            for flow in range(len(Flow_df_B)):\n",
    "\n",
    "                source = Flow_df_B[\"Start\"][flow]\n",
    "                sink = Flow_df_B[\"Target\"][flow]\n",
    "\n",
    "                flow_value, flow_dict = nx.maximum_flow(G, source, sink, capacity='capactiy')\n",
    "                subject_flow.append(flow_value)\n",
    "\n",
    "            Flow_df_B[subject] = np.transpose(subject_flow)\n",
    "            Flow_df_B[\"Condition\"] = condition\n",
    "            Flow_df_B[\"Direction\"] = \"B\" \n",
    "            \n",
    "            if condition == \"Dyadic\":\n",
    "                Flow_df_B[\"Leader\"] = subject_data[subject_data.SubjectID == subject]['Leader?'].values[0].copy()\n",
    "\n",
    "            \n",
    "            source_degree = centrality_df[centrality_df.Subject == str(subject)][source].item()\n",
    "            sink_degree = centrality_df[centrality_df.Subject == str(subject)][sink].item()\n",
    "            \n",
    "            min_degree_start_target_list.append(min(source_degree, sink_degree))\n",
    "            flow_list.append(flow_value)\n",
    "            \n",
    "            \n",
    "            \n",
    "    Flow_df_condition = pd.DataFrame()\n",
    "    Flow_df_condition = pd.concat([Flow_df_A, Flow_df_B], axis=0)\n",
    "    \n",
    "    Flow_df = pd.concat([Flow_df, Flow_df_condition])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_bool = False\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(flow_list, min_degree_start_target_list, 100, color=blue)\n",
    "\n",
    "ax2 = plt.gca()\n",
    "ax2.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "ax2.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "\n",
    "plt.xlabel('Flow', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Minimum Degree of start or target building', fontsize=20, fontweight='bold')\n",
    "\n",
    "if save_bool == True:\n",
    "    plt.savefig(GIT_GRAPH_PATH + \"Degree_vs_Flow_Scatter.png\",\n",
    "                dpi=200,\n",
    "                format=\"PNG\",\n",
    "                transparent=False,\n",
    "                facecolor='white',\n",
    "                bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the flow graphs of individual paths on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_wanted = True\n",
    "plotting_wanted = True\n",
    "\n",
    "close_figures = True\n",
    "\n",
    "# Select the condition\n",
    "conditions = ['Single']\n",
    "directions = ['A','B']\n",
    "\n",
    "edge_colors = ['r','g','b','y','m','c','k','grey',darkblue]\n",
    "\n",
    "\n",
    "for condition in conditions:\n",
    "    PROCESSED_DATA_PATH = './Results/' + condition + '/'\n",
    "    PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "\n",
    "\n",
    "    # open the subject info file of the condition\n",
    "    with open(PROCESSED_DATA_PATH + condition + '_Performance_Analysis.csv') as f:\n",
    "        try:\n",
    "            subject_data = pd.read_csv(f)\n",
    "        except:\n",
    "                print(\"\\tCould not load subject info - \" + str(condition) + \"!\")\n",
    "\n",
    "    subIDs = subject_data.SubjectID.values\n",
    "\n",
    "    #subIDs = [1074]\n",
    "    \n",
    "    subcount = 0\n",
    "\n",
    "    for subject in subIDs:\n",
    "        subcount +=1\n",
    "        print('Subject ' + str(subject) + ' started - ' + str(subcount) + '/' + str(len(subIDs)))\n",
    "\n",
    "        # get the data files according to the subject\n",
    "        subject_folder = sorted([f for f in GIT_GRAPH_FOLDER \n",
    "                                 if f.startswith(str(subject)+'_edgelist')], key=str.lower)\n",
    "\n",
    "        if len(subject_folder) != 0:\n",
    "\n",
    "            # open the JSON file as dictionary\n",
    "            with open(GIT_GRAPH_PATH + subject_folder[0]) as f:\n",
    "                try:\n",
    "                    edge_list = pd.read_csv(f)\n",
    "                except:\n",
    "                        print(\"\\tCould not load subject \" + str(subject) + \" edgelist!\")\n",
    "\n",
    "        else:\n",
    "            print('Subject ' + str(subject) + ' has no data file!')\n",
    "            continue \n",
    "\n",
    "        # --------- GRAPH CREATION ---------\n",
    "\n",
    "        # create graph from edgelist\n",
    "        G = nx.Graph()\n",
    "        G = nx.from_pandas_edgelist(edge_list, 'Edge1', 'Edge2')\n",
    "\n",
    "        # Remove the NoHit Node\n",
    "        G.remove_node('NoHouse')\n",
    "\n",
    "        # Remove the NoHit Node\n",
    "        G.remove_node('NoHit')\n",
    "        # Setting the node coordinates of each node of the graph\n",
    "\n",
    "\n",
    "        # node list\n",
    "        nodelist = list(G.nodes)\n",
    "        nodearray = np.array(G.nodes)\n",
    "\n",
    "\n",
    "        # coord dict\n",
    "        node_pos = {}\n",
    "\n",
    "        for node in nodelist:\n",
    "            # assign node coordinates\n",
    "            x = houselist['transformed_collidercenter_x'][houselist.target_collider_name==node].values[0]\n",
    "            y = houselist['transformed_collidercenter_y'][houselist.target_collider_name==node].values[0]\n",
    "            node_pos[node] = (x,y) \n",
    "\n",
    "        # set the graph's node coordinates attribute\n",
    "        nx.set_node_attributes(G, node_pos, 'coord')\n",
    "\n",
    "        # Set flow capacity of every edge to 1\n",
    "        nx.set_edge_attributes(G, 1.0, 'capactiy')\n",
    "\n",
    "        # degree dict and list of the graph\n",
    "        degree_dict = dict(G.degree)\n",
    "        degree_list = list(degree_dict.values())\n",
    "\n",
    "\n",
    "        # --------- MAX FLOW CALCULATION ---------  \n",
    "\n",
    "        # condition A\n",
    "        if subject_data.Condition[subject_data.SubjectID == subject].values[0] == 'A':\n",
    "\n",
    "            # Prepation         \n",
    "            sources = dests_A[:-1]\n",
    "            sinks = dests_A[1:]\n",
    "\n",
    "\n",
    "        # condition B\n",
    "        elif subject_data.Condition[subject_data.SubjectID == subject].values[0] == 'B':\n",
    "\n",
    "            # Prepation         \n",
    "            sources = dests_B[:-1]\n",
    "            sinks = dests_B[1:]  \n",
    "\n",
    "            \n",
    "        figgy = plt.figure(figsize=(16,15))\n",
    "\n",
    "\n",
    "        for path in range(len(sources)):\n",
    "\n",
    "            flow_dict_out = dict()\n",
    "            flow_edges = []\n",
    "\n",
    "            source = sources[path]\n",
    "            sink = sinks[path]\n",
    "\n",
    "            flow_value, flow_dict = nx.maximum_flow(G, source, sink, capacity='capactiy')\n",
    "\n",
    "            for a,b in flow_dict.items():\n",
    "                temp = {x:y for x,y in b.items() if y!=0}\n",
    "\n",
    "                if temp!={}:\n",
    "                    flow_dict_out[a] = temp\n",
    "\n",
    "                    for i in temp.keys():\n",
    "                        flow_edges.append((a,i))\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "\n",
    "            if plotting_wanted == True:\n",
    "                # Map Plot Preparation\n",
    "                # condition B\n",
    "                if subject_data.Condition[subject_data.SubjectID == subject].values[0] == 'B':\n",
    "                    ax3 = figgy.add_subplot(3,3,len(sources)-path)\n",
    "                else:\n",
    "                    ax3 = figgy.add_subplot(3,3,path+1)\n",
    "\n",
    "                # plot the map\n",
    "                plt.xlim(0, 4096)\n",
    "                plt.ylim(0, 4096)\n",
    "                ax3.set_frame_on(False)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(white_bg_img,aspect=ax3.get_aspect(),\n",
    "                         extent= ax3.get_xlim() + ax3.get_ylim(),\n",
    "                         zorder=1, alpha=0.3)\n",
    "\n",
    "                nx.draw_networkx_nodes(G,\n",
    "                                       node_pos, \n",
    "                                       alpha = 1, \n",
    "                                       node_size = 7, \n",
    "                                       cmap=parula_map)\n",
    "\n",
    "                nx.draw_networkx_nodes(G,\n",
    "                                       node_pos,\n",
    "                                       nodelist=[source],\n",
    "                                       alpha = 1, \n",
    "                                       node_size = 100, \n",
    "                                       node_color='g')\n",
    "\n",
    "                nx.draw_networkx_nodes(G,\n",
    "                                       node_pos,\n",
    "                                       nodelist=[sink],\n",
    "                                       alpha = 1, \n",
    "                                       node_size = 100, \n",
    "                                       node_color='r')\n",
    "\n",
    "\n",
    "\n",
    "                nx.draw_networkx_edges(G, \n",
    "                                       node_pos, \n",
    "                                       edgelist=flow_edges,\n",
    "                                       edge_color='k', \n",
    "                                       alpha=1, \n",
    "                                       width=2,\n",
    "                                       style='dashed')\n",
    "\n",
    "                ax3.set_xlim(0,3800)\n",
    "                ax3.set_ylim(300,3700)\n",
    "\n",
    "                ax3.set_title(r\"$\\bf{\" + 'Path ' + str(path+1) + ': ' + \"}$\" + source + ' to ' + sink, fontsize=12.3)\n",
    "            \n",
    "            \n",
    "        if saving_wanted == True:\n",
    "\n",
    "            # saving the figure\n",
    "            try:\n",
    "                plt.savefig(GIT_GRAPH_PATH + str(subject) + \"_FlowGraph.png\",\n",
    "                            dpi=200,\n",
    "                            format=\"PNG\",\n",
    "                            transparent=False,\n",
    "                            facecolor='white',\n",
    "                            bbox_inches = \"tight\")\n",
    "            except:\n",
    "                print(\"\\tCould not save subject \" + str(subject) + \" FlowGraph as PNG!\")\n",
    "\n",
    "        if close_figures == True:\n",
    "            plt.close()\n",
    "    \n",
    "print('Done')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Buildings on paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saving_wanted = False\n",
    "plotting_wanted = False\n",
    "\n",
    "# Select the condition\n",
    "conditions = ['Single', 'SingleC', 'Dyadic']\n",
    "directions = ['A','B']\n",
    "\n",
    "edge_colors = ['r','g','b','y','m','c','k','grey',darkblue]\n",
    "\n",
    "# Dictionary for all flow buildings on paths\n",
    "Flow_buildings_dict = {'Single': {'A':{},'B':{}},\n",
    "                       'SingleC':{'A':{},'B':{}},\n",
    "                       'Dyadic':{'A':{},'B':{}}\n",
    "                      }\n",
    "\n",
    "\n",
    "for path in range(9):\n",
    "    print('Path ' + str(path+1) + ' started')\n",
    "\n",
    "        \n",
    "    path_success_str = \"P{}:Success\".format(path+1)\n",
    "        \n",
    "    for condition in conditions:\n",
    "        PROCESSED_DATA_PATH = './Results/' + condition + '/'\n",
    "        PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "\n",
    "\n",
    "        # open the subject info file of the condition\n",
    "        with open(PROCESSED_DATA_PATH + condition + '_Performance_Analysis.csv') as f:\n",
    "            try:\n",
    "                subject_data = pd.read_csv(f)\n",
    "            except:\n",
    "                    print(\"\\tCould not load subject info - \" + str(condition) + \"!\")\n",
    "\n",
    "                    \n",
    "        Flow_buildings_dict[condition]['A'][path] = {}\n",
    "        Flow_buildings_dict[condition]['B'][path] = {}\n",
    "\n",
    "        flow_buildings_A_success = []\n",
    "        flow_buildings_B_success = []\n",
    "        \n",
    "        flow_buildings_A_fail = []\n",
    "        flow_buildings_B_fail = []\n",
    "        \n",
    "        subIDs = subject_data.SubjectID.values\n",
    "\n",
    "        #subIDs = [subIDs[0]]\n",
    "\n",
    "        subcount = 0\n",
    "\n",
    "        for subject in subIDs:\n",
    "            subcount +=1\n",
    "            \n",
    "            #print('\\tSubject ' + str(subject) + ' started - ' + str(subcount) + '/' + str(len(subIDs)))\n",
    "            # get the data files according to the subject\n",
    "            subject_folder = sorted([f for f in GIT_GRAPH_FOLDER \n",
    "                                     if f.startswith(str(subject)+'_edgelist')], key=str.lower)\n",
    "\n",
    "            if len(subject_folder) != 0:\n",
    "\n",
    "                # open the JSON file as dictionary\n",
    "                with open(GIT_GRAPH_PATH + subject_folder[0]) as f:\n",
    "                    try:\n",
    "                        edge_list = pd.read_csv(f)\n",
    "                    except:\n",
    "                            print(\"\\tCould not load subject \" + str(subject) + \" edgelist!\")\n",
    "\n",
    "            else:\n",
    "                print('Subject ' + str(subject) + ' has no data file!')\n",
    "                continue \n",
    "\n",
    "            # --------- GRAPH CREATION ---------\n",
    "\n",
    "            # create graph from edgelist\n",
    "            G = nx.Graph()\n",
    "            G = nx.from_pandas_edgelist(edge_list, 'Edge1', 'Edge2')\n",
    "\n",
    "            # Remove the NoHit Node\n",
    "            G.remove_node('NoHouse')\n",
    "\n",
    "            # Remove the NoHit Node\n",
    "            G.remove_node('NoHit')\n",
    "            # Setting the node coordinates of each node of the graph\n",
    "\n",
    "\n",
    "            # node list\n",
    "            nodelist = list(G.nodes)\n",
    "            nodearray = np.array(G.nodes)\n",
    "\n",
    "\n",
    "            # coord dict\n",
    "            node_pos = {}\n",
    "\n",
    "            for node in nodelist:\n",
    "                # assign node coordinates\n",
    "                x = houselist['transformed_collidercenter_x'][houselist.target_collider_name==node].values[0]\n",
    "                y = houselist['transformed_collidercenter_y'][houselist.target_collider_name==node].values[0]\n",
    "                node_pos[node] = (x,y) \n",
    "\n",
    "            # set the graph's node coordinates attribute\n",
    "            nx.set_node_attributes(G, node_pos, 'coord')\n",
    "\n",
    "            # Set flow capacity of every edge to 1\n",
    "            nx.set_edge_attributes(G, 1.0, 'capactiy')\n",
    "\n",
    "            # degree dict and list of the graph\n",
    "            degree_dict = dict(G.degree)\n",
    "            degree_list = list(degree_dict.values())\n",
    "\n",
    "\n",
    "            # condition A\n",
    "            if subject_data.Condition[subject_data.SubjectID == subject].values[0] == 'A':\n",
    "\n",
    "                # Prepation         \n",
    "                sources = dests_A[:-1]\n",
    "                sinks = dests_A[1:]\n",
    "\n",
    "                flow_dict_out = dict()\n",
    "               \n",
    "\n",
    "                source = sources[path]\n",
    "                sink = sinks[path]\n",
    "\n",
    "                flow_value, flow_dict = nx.maximum_flow(G, source, sink, capacity='capactiy')\n",
    "\n",
    "                for a,b in flow_dict.items():\n",
    "                    temp = {x:y for x,y in b.items() if y!=0}\n",
    "\n",
    "                    if temp!={}:\n",
    "                        flow_dict_out[a] = temp\n",
    "\n",
    "                        for i in temp.keys():\n",
    "                            if subject_data[path_success_str][subject_data.SubjectID == subject].values[0] == True:\n",
    "                                flow_buildings_A_success.append(a)\n",
    "                                flow_buildings_A_success.append(i)\n",
    "                            else:\n",
    "                                flow_buildings_A_fail.append(a)\n",
    "                                flow_buildings_A_fail.append(i)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "\n",
    "\n",
    "            # condition B\n",
    "            elif subject_data.Condition[subject_data.SubjectID == subject].values[0] == 'B':\n",
    "\n",
    "                # Prepation         \n",
    "                sources = dests_B[:-1]\n",
    "                sinks = dests_B[1:]  \n",
    "\n",
    "                flow_dict_out = dict()\n",
    "                \n",
    "\n",
    "                source = sources[path]\n",
    "                sink = sinks[path]\n",
    "\n",
    "                flow_value, flow_dict = nx.maximum_flow(G, source, sink, capacity='capactiy')\n",
    "\n",
    "                for a,b in flow_dict.items():\n",
    "                    temp = {x:y for x,y in b.items() if y!=0}\n",
    "\n",
    "                    if temp!={}:\n",
    "                        flow_dict_out[a] = temp\n",
    "\n",
    "                        for i in temp.keys():\n",
    "                            if subject_data[path_success_str][subject_data.SubjectID == subject].values[0] == True:\n",
    "                                flow_buildings_B_success.append(a)\n",
    "                                flow_buildings_B_success.append(i)\n",
    "                            else:\n",
    "                                flow_buildings_B_fail.append(a)\n",
    "                                flow_buildings_B_fail.append(i)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "\n",
    "        Flow_buildings_dict[condition]['A'][path]['Success'] = np.unique(flow_buildings_A_success)\n",
    "        Flow_buildings_dict[condition]['B'][path]['Success'] = np.unique(flow_buildings_B_success)\n",
    "        Flow_buildings_dict[condition]['A'][path]['Fail'] = np.unique(flow_buildings_A_fail)\n",
    "        Flow_buildings_dict[condition]['B'][path]['Fail'] = np.unique(flow_buildings_B_fail)\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dictionary for all flow buildings on paths\n",
    "Degree_flow_buildings_dict = {'Single': {'A':{},'B':{}},\n",
    "                              'SingleC':{'A':{},'B':{}},\n",
    "                              'Dyadic':{'A':{},'B':{}}\n",
    "                             }\n",
    "\n",
    "for condition in conditions:\n",
    "    for direction in directions:\n",
    "        for path, buildings in Flow_buildings_dict[condition][direction].items():\n",
    "            \n",
    "            Degree_flow_buildings_dict[condition][direction][path] = {}\n",
    "            \n",
    "            # Success\n",
    "            Degree_flow_buildings_dict[condition][direction][path]['Success'] = \\\n",
    "                {'NumberOfHouses':{len(buildings['Success'])},\n",
    "                'MeanDegree':{centrality_df[buildings['Success']].mean().mean()},\n",
    "                'STDDegree':{centrality_df[buildings['Success']].std().mean()}}\n",
    "           \n",
    "            # Failure\n",
    "            Degree_flow_buildings_dict[condition][direction][path]['Fail'] = \\\n",
    "                {'NumberOfHouses':{len(buildings['Fail'])},\n",
    "                'MeanDegree':{centrality_df[buildings['Fail']].mean().mean()},\n",
    "                'STDDegree':{centrality_df[buildings['Fail']].std().mean()}}            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(15,15))\n",
    "\n",
    "fig, ((ax1, ax3), (ax5, ax7)) = plt.subplots(2, 2, figsize=(25,18))\n",
    "\n",
    "ax2=ax1.twinx()\n",
    "ax4=ax3.twinx()\n",
    "ax6=ax5.twinx()\n",
    "ax8=ax7.twinx()\n",
    "\n",
    "labels = []\n",
    "\n",
    "# loop through conditions and directions\n",
    "for condition in conditions:\n",
    "    for direction in directions:\n",
    "        \n",
    "        # init degree list\n",
    "        mean_degrees_success = []\n",
    "        std_degrees_success = []\n",
    "        numberOfHouses_success = []\n",
    "        \n",
    "        mean_degrees_fail = []\n",
    "        std_degrees_fail = []\n",
    "        numberOfHouses_fail = []\n",
    "        \n",
    "        for path in Degree_flow_buildings_dict[condition][direction]:\n",
    "            \n",
    "            # Success\n",
    "            mean_degrees_success.append(list(Degree_flow_buildings_dict[condition][direction][path]['Success']['MeanDegree'])[0])\n",
    "            std_degrees_success.append(list(Degree_flow_buildings_dict[condition][direction][path]['Success']['STDDegree'])[0])\n",
    "            numberOfHouses_success.append(list(Degree_flow_buildings_dict[condition][direction][path]['Success']['NumberOfHouses'])[0])\n",
    "            # Fail\n",
    "            mean_degrees_fail.append(list(Degree_flow_buildings_dict[condition][direction][path]['Fail']['MeanDegree'])[0])\n",
    "            std_degrees_fail.append(list(Degree_flow_buildings_dict[condition][direction][path]['Fail']['STDDegree'])[0])\n",
    "            numberOfHouses_fail.append(list(Degree_flow_buildings_dict[condition][direction][path]['Fail']['NumberOfHouses'])[0])\n",
    "        \n",
    "        \n",
    "        if direction == 'A':\n",
    "            ax1.errorbar(x=range(0,9), y=mean_degrees_success, yerr=std_degrees_success)\n",
    "            ax2.scatter(range(0,9), numberOfHouses_success)\n",
    "            \n",
    "            ax3.errorbar(x=range(0,9), y=mean_degrees_fail, yerr=std_degrees_fail)\n",
    "            ax4.scatter(range(0,9), numberOfHouses_fail)\n",
    "            \n",
    "        else:\n",
    "            ax5.errorbar(x=range(0,9), y=mean_degrees_success[::-1], yerr=std_degrees_success[::-1])\n",
    "            ax6.scatter(range(0,9), numberOfHouses_success[::-1])\n",
    "            \n",
    "            ax7.errorbar(x=range(0,9), y=mean_degrees_fail[::-1], yerr=std_degrees_fail[::-1])\n",
    "            ax8.scatter(range(0,9), numberOfHouses_fail[::-1])\n",
    "            \n",
    "        ax1.set_title('Mean degree of flow houses - Direction A - Success', fontsize=20)\n",
    "        ax1.set_ylim([3,17])\n",
    "        ax1.set_xlabel('Path', fontsize=20, fontweight='bold')\n",
    "        ax1.set_ylabel('Degree', fontsize=20, fontweight='bold')\n",
    "        ax1.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "        ax1.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "\n",
    "        ax2.set_ylabel('Number of Houses', fontsize=20)\n",
    "        ax2.set_ylim([0,250])\n",
    "        ax2.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "        ax2.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "        ax3.set_title('Mean degree of flow houses - Direction A - Failure', fontsize=20)\n",
    "        ax3.set_ylim([3,17])\n",
    "        ax3.set_xlabel('Path', fontsize=20, fontweight='bold')\n",
    "        ax3.set_ylabel('Degree', fontsize=20, fontweight='bold') \n",
    "        ax3.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "        ax3.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "        ax4.set_ylabel('Number of Houses', fontsize=20)\n",
    "        ax4.set_ylim([0,250])\n",
    "        ax4.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "        ax4.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "        \n",
    "        ax5.set_title('Mean degree of flow houses - Direction B - Success', fontsize=20)\n",
    "        ax5.set_ylim([3,17])\n",
    "        ax5.set_xlabel('Path', fontsize=20, fontweight='bold')\n",
    "        ax5.set_ylabel('Degree', fontsize=20, fontweight='bold')\n",
    "        ax5.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "        ax5.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "        ax6.set_ylabel('Number of Houses', fontsize=20)\n",
    "        ax6.set_ylim([0,250])\n",
    "        ax6.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "        ax6.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "        ax7.set_title('Mean degree of flow houses - Direction B - Failure', fontsize=20)\n",
    "        ax7.set_ylim([3,17])\n",
    "        ax7.set_xlabel('Path', fontsize=20, fontweight='bold')\n",
    "        ax7.set_ylabel('Degree', fontsize=20, fontweight='bold') \n",
    "        ax7.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "        ax7.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "        ax8.set_ylabel('Number of Houses', fontsize=20)\n",
    "        ax8.set_ylim([0,250])\n",
    "        ax8.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "        ax8.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "        \n",
    "    labels.append(condition)\n",
    "        \n",
    "ax1.legend(labels)\n",
    "ax3.legend(labels)\n",
    "ax5.legend(labels)\n",
    "ax7.legend(labels)\n",
    "       \n",
    "    \n",
    "plt.savefig(GIT_GRAPH_PATH + \"Degree_amountofflowbuildings_errorbar.png\",\n",
    "            dpi=200,\n",
    "            format=\"PNG\",\n",
    "            transparent=False,\n",
    "            facecolor='white',\n",
    "            bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "#if close_figures == True:\n",
    "#    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree based analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the condition\n",
    "conditions = ['Single', 'SingleC' , 'Dyadic']\n",
    "directions = ['A','B']\n",
    "\n",
    "Target_df = pd.DataFrame()\n",
    "Degree_df = pd.DataFrame()\n",
    "Success_df = pd.DataFrame()\n",
    "\n",
    "for condition in conditions:\n",
    "    PROCESSED_DATA_PATH = './Results/' + condition + '/'\n",
    "    PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "\n",
    "\n",
    "    # open the subject info file of the condition\n",
    "    with open(PROCESSED_DATA_PATH + condition + '_Performance_Analysis.csv') as f:\n",
    "        try:\n",
    "            subject_data = pd.read_csv(f)\n",
    "        except:\n",
    "                print(\"\\tCould not load subject info - \" + str(condition) + \"!\")\n",
    "\n",
    "    subIDs = subject_data.SubjectID.values\n",
    "\n",
    "    subcount = 0\n",
    "\n",
    "    Target_df_A = pd.DataFrame(columns=[\"Start\", \"Target\", \"Condition\", \"Direction\", \"Leader\"])\n",
    "    Target_df_B = Target_df_A.copy()\n",
    "    \n",
    "    Degree_df_A = pd.DataFrame(columns=[\"Start\", \"Target\", \"Condition\", \"Direction\", \"Leader\"])\n",
    "    Degree_df_B = Degree_df_A.copy()\n",
    "    \n",
    "    Success_df_A = pd.DataFrame(columns=[\"Start\", \"Target\", \"Condition\", \"Direction\", \"Leader\"])\n",
    "    Success_df_B = Success_df_A.copy()\n",
    "\n",
    "\n",
    "\n",
    "    for subject in subIDs:\n",
    "        subcount +=1\n",
    "        print('Subject ' + str(subject) + ' started - ' + str(subcount) + '/' + str(len(subIDs)))\n",
    "\n",
    "        # get the data files according to the subject\n",
    "        subject_folder = sorted([f for f in GIT_GRAPH_FOLDER \n",
    "                                 if f.startswith(str(subject)+'_edgelist')], key=str.lower)\n",
    "\n",
    "        if len(subject_folder) != 0:\n",
    "\n",
    "            # open the JSON file as dictionary\n",
    "            with open(GIT_GRAPH_PATH + subject_folder[0]) as f:\n",
    "                try:\n",
    "                    edge_list = pd.read_csv(f)\n",
    "                except:\n",
    "                        print(\"\\tCould not load subject \" + str(subject) + \" edgelist!\")\n",
    "\n",
    "        else:\n",
    "            print('Subject ' + str(subject) + ' has no data file!')\n",
    "            continue \n",
    "\n",
    "\n",
    "\n",
    "        # --------- GRAPH CREATION ---------\n",
    "\n",
    "        # create graph from edgelist\n",
    "        G = nx.Graph()\n",
    "        G = nx.from_pandas_edgelist(edge_list, 'Edge1', 'Edge2')\n",
    "\n",
    "        # Remove the NoHit Node\n",
    "        G.remove_node('NoHouse')\n",
    "\n",
    "        # Remove the NoHit Node\n",
    "        G.remove_node('NoHit')\n",
    "        # Setting the node coordinates of each node of the graph\n",
    "\n",
    "\n",
    "        # node list\n",
    "        nodelist = list(G.nodes)\n",
    "        nodearray = np.array(G.nodes)\n",
    "\n",
    "\n",
    "        # coord dict\n",
    "        node_pos = {}\n",
    "\n",
    "        for node in nodelist:\n",
    "            # assign node coordinates\n",
    "            x = houselist['transformed_collidercenter_x'][houselist.target_collider_name==node].values[0]\n",
    "            y = houselist['transformed_collidercenter_y'][houselist.target_collider_name==node].values[0]\n",
    "            node_pos[node] = (x,y) \n",
    "\n",
    "        # set the graph's node coordinates attribute\n",
    "        nx.set_node_attributes(G, node_pos, 'coord')\n",
    "\n",
    "        # Set flow capacity of every edge to 1\n",
    "        nx.set_edge_attributes(G, 1.0, 'capactiy')\n",
    "\n",
    "        # degree dict and list of the graph\n",
    "        degree_dict = dict(G.degree)\n",
    "        degree_list = list(degree_dict.values())\n",
    "\n",
    "\n",
    "        # --------- DEGREE CALCULATION ---------  \n",
    "\n",
    "        # condition A\n",
    "        if subject_data.Condition[subject_data.SubjectID == subject].values[0] == 'A':\n",
    "\n",
    "            Target_df_A[\"Start\"] = dests_A[:-1]\n",
    "            Target_df_A[\"Target\"].loc[0:len(dests_A)] = dests_A[1:]\n",
    "            \n",
    "            Degree_df_A[\"Start\"] = dests_A[:-1]\n",
    "            Degree_df_A[\"Target\"].loc[0:len(dests_A)] = dests_A[1:]\n",
    "            \n",
    "            Success_df_A[\"Start\"] = dests_A[:-1]\n",
    "            Success_df_A[\"Target\"].loc[0:len(dests_A)] = dests_A[1:]\n",
    "            \n",
    "            subject_degree = []\n",
    "            subject_success = []\n",
    "            target_degree_temp = []\n",
    "\n",
    "            for degree in range(len(Degree_df_A)):\n",
    "\n",
    "                # Degree Part\n",
    "                start = Degree_df_A[\"Start\"][degree]\n",
    "                target = Degree_df_A[\"Target\"][degree]\n",
    "\n",
    "                start_degree = centrality_df[centrality_df.Subject == str(subject)][start].values[0]\n",
    "                target_degree = centrality_df[centrality_df.Subject == str(subject)][target].values[0]\n",
    "\n",
    "                combined_degree = (start_degree, target_degree)\n",
    "                \n",
    "                subject_degree.append(combined_degree)\n",
    "                target_degree_temp.append(target_degree)\n",
    "                \n",
    "                # Success Part\n",
    "                pathstr = \"P{}:Success\".format(degree+1)\n",
    "                subject_success.append(subject_data[subject_data.SubjectID == subject][pathstr].item())\n",
    "            \n",
    "            \n",
    "            Target_df_A[subject] = pd.Series(target_degree_temp)\n",
    "            Target_df_A[\"Condition\"] = condition\n",
    "            Target_df_A[\"Direction\"] = \"A\" \n",
    "\n",
    "            Degree_df_A[subject] = pd.Series(subject_degree)\n",
    "            Degree_df_A[\"Condition\"] = condition\n",
    "            Degree_df_A[\"Direction\"] = \"A\"\n",
    "            \n",
    "            #Success \n",
    "            Success_df_A[subject] = pd.Series(subject_success)\n",
    "            Success_df_A[\"Condition\"] = condition\n",
    "            Success_df_A[\"Direction\"] = \"A\"\n",
    "            \n",
    "            if condition == \"Dyadic\":\n",
    "                Target_df_A[\"Leader\"] = subject_data[subject_data.SubjectID == subject]['Leader?'].values[0].copy()\n",
    "                Degree_df_A[\"Leader\"] = subject_data[subject_data.SubjectID == subject]['Leader?'].values[0].copy()\n",
    "                Success_df_A[\"Leader\"] = subject_data[subject_data.SubjectID == subject]['Leader?'].values[0].copy()\n",
    "\n",
    "\n",
    "\n",
    "        # condition B\n",
    "        elif subject_data.Condition[subject_data.SubjectID == subject].values[0] == 'B':\n",
    "            \n",
    "            Target_df_B[\"Start\"] = dests_B[:-1]\n",
    "            Target_df_B[\"Target\"].loc[0:len(dests_A)] = dests_B[1:]\n",
    "            \n",
    "            Degree_df_B[\"Start\"] = dests_B[:-1]\n",
    "            Degree_df_B[\"Target\"].loc[0:len(dests_B)] = dests_B[1:]    \n",
    "\n",
    "            Success_df_B[\"Start\"] = dests_B[:-1]\n",
    "            Success_df_B[\"Target\"].loc[0:len(dests_B)] = dests_B[1:]\n",
    "            \n",
    "            subject_degree = []\n",
    "            subject_success = []\n",
    "            target_degree_temp = []\n",
    "\n",
    "            for degree in range(len(Degree_df_B)):\n",
    "                \n",
    "                start = Degree_df_B[\"Start\"][degree]\n",
    "                target = Degree_df_B[\"Target\"][degree]\n",
    "\n",
    "                start_degree = centrality_df[centrality_df.Subject == str(subject)][start].values[0]\n",
    "                target_degree = centrality_df[centrality_df.Subject == str(subject)][target].values[0]\n",
    "\n",
    "                combined_degree = (start_degree, target_degree)\n",
    "                \n",
    "                subject_degree.append(combined_degree)\n",
    "                target_degree_temp.append(target_degree)\n",
    "                \n",
    "                # Success Part\n",
    "                pathstr = \"P{}:Success\".format(degree+1)\n",
    "                subject_success.append(subject_data[subject_data.SubjectID == subject][pathstr].item())\n",
    "                \n",
    "            Target_df_B[subject] = pd.Series(target_degree_temp)\n",
    "            Target_df_B[\"Condition\"] = condition\n",
    "            Target_df_B[\"Direction\"] = \"B\" \n",
    "\n",
    "            Degree_df_B[subject] = pd.Series(subject_degree)\n",
    "            Degree_df_B[\"Condition\"] = condition\n",
    "            Degree_df_B[\"Direction\"] = \"B\" \n",
    "            \n",
    "            #Success \n",
    "            Success_df_B[subject] = pd.Series(subject_success)\n",
    "            Success_df_B[\"Condition\"] = condition\n",
    "            Success_df_B[\"Direction\"] = \"B\"\n",
    "            \n",
    "            if condition == \"Dyadic\":\n",
    "                Target_df_B[\"Leader\"] = subject_data[subject_data.SubjectID == subject]['Leader?'].values[0].copy()\n",
    "                Degree_df_B[\"Leader\"] = subject_data[subject_data.SubjectID == subject]['Leader?'].values[0].copy()\n",
    "                Success_df_B[\"Leader\"] = subject_data[subject_data.SubjectID == subject]['Leader?'].values[0].copy()\n",
    "    \n",
    "    Target_df_condition = pd.DataFrame()\n",
    "    Target_df_condition = pd.concat([Target_df_A, Target_df_B], axis=0)\n",
    "    \n",
    "    Target_df = pd.concat([Target_df, Target_df_condition])\n",
    "                \n",
    "              \n",
    "    Degree_df_condition = pd.DataFrame()\n",
    "    Degree_df_condition = pd.concat([Degree_df_A, Degree_df_B], axis=0)\n",
    "    \n",
    "    Degree_df = pd.concat([Degree_df, Degree_df_condition])\n",
    "    \n",
    "    Success_df_condition = pd.DataFrame()\n",
    "    Success_df_condition = pd.concat([Success_df_A, Success_df_B], axis=0)\n",
    "    \n",
    "    Success_df = pd.concat([Success_df, Success_df_condition])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['Single', 'SingleC', 'Dyadic']\n",
    "directions = ['A','B']\n",
    "\n",
    "conditions1 = ['Single',  'Single', 'SingleC']\n",
    "conditions2 = ['SingleC', 'Dyadic', 'Dyadic']\n",
    "\n",
    "directions1 = ['A','A','B','B']\n",
    "directions2 = ['A','B','A','B']\n",
    "\n",
    "\n",
    "# create correlation table\n",
    "Flow_means_df = pd.DataFrame(columns=['Condition1', \n",
    "                                      'Direction1', \n",
    "                                      'Condition2', \n",
    "                                      'Direction2', \n",
    "                                      'Correlation'])\n",
    "\n",
    "\n",
    "\n",
    "pd_idx = 0\n",
    "\n",
    "\n",
    "\n",
    "for c_idx in range(len(conditions1)):\n",
    "    for d_idx in range(len(directions1)):\n",
    "             \n",
    "            condition1 = conditions1[c_idx]\n",
    "            condition2 = conditions2[c_idx]\n",
    "\n",
    "            direction1 = directions1[d_idx]\n",
    "            direction2 = directions2[d_idx]\n",
    "            \n",
    "            Flow_means_df.loc[pd_idx, 'Condition1'] = condition1\n",
    "            Flow_means_df.loc[pd_idx, 'Direction1'] = direction1\n",
    "            Flow_means_df.loc[pd_idx, 'Condition2'] = condition2\n",
    "            Flow_means_df.loc[pd_idx, 'Direction2'] = direction2\n",
    "            \n",
    "            \n",
    "            if direction1 == direction2:\n",
    "                value1 = Flow_df.loc[(Flow_df.Condition==condition1) & (Flow_df['Direction']==direction1)].dropna(axis=1).mean(axis=1).copy()\n",
    "                value2 = Flow_df.loc[(Flow_df.Condition==condition2) & (Flow_df['Direction']==direction2)].dropna(axis=1).mean(axis=1).copy()\n",
    "                \n",
    "                std1 = Flow_df.loc[(Flow_df.Condition==condition1) & (Flow_df['Direction']==direction1)].dropna(axis=1).std(axis=1).copy()\n",
    "                std2 = Flow_df.loc[(Flow_df.Condition==condition2) & (Flow_df['Direction']==direction2)].dropna(axis=1).std(axis=1).copy()\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                value1 = Flow_df.loc[(Flow_df.Condition==condition1) & (Flow_df['Direction']==direction1)].dropna(axis=1).mean(axis=1).copy()\n",
    "                value2 = Flow_df.loc[(Flow_df.Condition==condition2) & (Flow_df['Direction']==direction2)].dropna(axis=1).mean(axis=1).copy()\n",
    "                value2 = value2.sort_index(ascending=False)\n",
    "                \n",
    "                std1 = Flow_df.loc[(Flow_df.Condition==condition1) & (Flow_df['Direction']==direction1)].dropna(axis=1).std(axis=1).copy()\n",
    "                std2 = Flow_df.loc[(Flow_df.Condition==condition2) & (Flow_df['Direction']==direction2)].dropna(axis=1).std(axis=1).copy()\n",
    "                \n",
    "                \n",
    "            Flow_means_df.loc[pd_idx, 'Correlation'] = np.corrcoef(value1, value2)[0,1]\n",
    "            \n",
    "            pd_idx +=1\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "figgy = plt.figure(figsize=(18,13))\n",
    "\n",
    "\n",
    "# create grid for different subplots\n",
    "gs = gridspec.GridSpec(ncols=2, nrows=2,\n",
    "                         width_ratios=[1, 1], wspace=0.3,\n",
    "                         hspace=0.3, height_ratios=[1, 1])\n",
    "\n",
    "\n",
    "ax1 = figgy.add_subplot(gs[0])\n",
    "\n",
    "#plt.subplot(2,2,1)\n",
    "plt.xlabel('Path', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Flow', fontsize=20, fontweight='bold')\n",
    "#plt.title('Flow Errorbars across paths')\n",
    "labels = []\n",
    "\n",
    "for con in conditions:\n",
    "    for di in directions:\n",
    "        \n",
    "        y = Flow_df.loc[(Flow_df.Condition==con) & (Flow_df['Direction']==di)].dropna(axis=1).mean(axis=1).copy()\n",
    "        yerr = Flow_df.loc[(Flow_df.Condition==con) & (Flow_df['Direction']==di)].dropna(axis=1).std(axis=1).copy()\n",
    "               \n",
    "        plt.errorbar(x=range(0,9), y=y, yerr=yerr)\n",
    "        \n",
    "        labels.append(con+'_'+di)\n",
    "        \n",
    "#plt.legend(labels)\n",
    "\n",
    "plt.legend(labels, loc=\"best\", ncol=3)\n",
    "\n",
    "ax1 = plt.gca()\n",
    "ax1.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "ax1.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "\n",
    "\n",
    "ax2 = figgy.add_subplot(gs[1])\n",
    "#plt.subplot(2,2,2)\n",
    "plt.xlabel('Path', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Success', fontsize=20, fontweight='bold')\n",
    "#plt.title('Success Errorbars across paths')\n",
    "labels = []\n",
    "\n",
    "for con in conditions:\n",
    "    for di in directions:\n",
    "        \n",
    "        succ = Success_df.loc[(Success_df.Condition==con) & (Success_df['Direction']==di)].copy()\n",
    "        succ = succ.drop([\"Start\", \"Target\",\"Condition\",\"Direction\",\"Leader\"], axis=1)\n",
    "        succ = succ.dropna(axis=1).copy()\n",
    "        y = succ.mean(axis=1).copy()\n",
    "\n",
    "        yerr = succ.std(axis=1).copy()\n",
    "        \n",
    "        plt.errorbar(x=range(0,9), y=y, yerr=yerr)\n",
    "        \n",
    "        labels.append(con+'_'+di)\n",
    "        \n",
    "#plt.legend(labels)\n",
    "\n",
    "\n",
    "ax2 = plt.gca()\n",
    "ax2.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "ax2.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "\n",
    "#directions = ['A', 'B']\n",
    "#\n",
    "#plt.subplot(2,2,2)\n",
    "#plt.hist(Flow_means_df['Correlation'], ec='k', color=blue)\n",
    "#plt.xlabel('CorrCoeff', fontsize=20, fontweight='bold')\n",
    "#plt.ylabel('Frequency', fontsize=20, fontweight='bold')\n",
    "#plt.title('Correlation between maxflows averaged over subjects per path')\n",
    "#\n",
    "#plt.subplot(2,2,3)\n",
    "ax3 = figgy.add_subplot(gs[2])\n",
    "\n",
    "plt.xlabel('Path', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Degree', fontsize=20, fontweight='bold')\n",
    "#plt.title('Degree Errorbars across paths')\n",
    "\n",
    "labels =[]\n",
    "for con in conditions:\n",
    "    for di in directions:\n",
    "    \n",
    "        if di == 'A':\n",
    "            lst = Degree_df.loc[(Degree_df.Condition==con) & (Degree_df['Direction']==di)].dropna(axis=1)\n",
    "            y = []\n",
    "            yerr = []\n",
    "            for idx in range(len(lst)):\n",
    "                if con == 'Dyadic':\n",
    "                    y.append(np.mean(list(map(lambda x: x[0], lst.iloc[idx, 5:]))))\n",
    "                    yerr.append(np.std(list(map(lambda x: x[0], lst.iloc[idx, 5:]))))\n",
    "                else:\n",
    "                    y.append(np.mean(list(map(lambda x: x[0], lst.iloc[idx, 4:]))))\n",
    "                    yerr.append(np.std(list(map(lambda x: x[0], lst.iloc[idx, 4:]))))\n",
    "\n",
    "\n",
    "            plt.errorbar(x=range(0,9), y=y, yerr=yerr)\n",
    "\n",
    "            labels.append(con+di)\n",
    "            \n",
    "        else:\n",
    "            lst = Degree_df.loc[(Degree_df.Condition==con) & (Degree_df['Direction']==di)].dropna(axis=1)\n",
    "            y = []\n",
    "            yerr = []\n",
    "            for idx in range(len(lst)):\n",
    "                if con == 'Dyadic':\n",
    "                    y.append(np.mean(list(map(lambda x: x[1], lst.iloc[idx, 5:]))))\n",
    "                    yerr.append(np.std(list(map(lambda x: x[1], lst.iloc[idx, 5:]))))\n",
    "                else:\n",
    "                    y.append(np.mean(list(map(lambda x: x[1], lst.iloc[idx, 4:]))))\n",
    "                    yerr.append(np.std(list(map(lambda x: x[1], lst.iloc[idx, 4:]))))\n",
    "\n",
    "\n",
    "            plt.errorbar(x=range(0,9), y=y, yerr=yerr)\n",
    "\n",
    "            labels.append(con+'_'+di)\n",
    "        \n",
    "#plt.legend(labels)\n",
    "\n",
    "ax3 = plt.gca()\n",
    "ax3.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "ax3.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "\n",
    "\n",
    "# ---- Difference between Buildings - A ----\n",
    "\n",
    "directions = ['A', 'B']\n",
    "\n",
    "#plt.subplot(2,2,4)\n",
    "\n",
    "ax4 = figgy.add_subplot(gs[3])\n",
    "\n",
    "plt.xlabel('Path', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Degree', fontsize=20, fontweight='bold')\n",
    "#plt.title('Degree Errorbars across paths - Difference between start and target')\n",
    "\n",
    "labels =[]\n",
    "\n",
    "for con in conditions:\n",
    "    for di in directions:\n",
    "    \n",
    "        lst = Degree_df.loc[(Degree_df.Condition==con) & (Degree_df['Direction']==di)].dropna(axis=1)\n",
    "        y = []\n",
    "        yerr = []\n",
    "        for idx in range(len(lst)):\n",
    "            if con == 'Dyadic':\n",
    "                y.append(np.mean(list(map(lambda x: abs(x[0]-x[1]), lst.iloc[idx, 5:]))))\n",
    "                yerr.append(np.std(list(map(lambda x: abs(x[0]-x[1]), lst.iloc[idx, 5:]))))\n",
    "            else:\n",
    "                y.append(np.mean(list(map(lambda x: abs(x[0]-x[1]), lst.iloc[idx, 4:]))))\n",
    "                yerr.append(np.std(list(map(lambda x: abs(x[0]-x[1]), lst.iloc[idx, 4:]))))\n",
    "      \n",
    "                        \n",
    "        plt.errorbar(x=range(0,9), y=y, yerr=yerr)\n",
    "        \n",
    "        labels.append(con+'_'+di)\n",
    "        \n",
    "#plt.legend(labels)\n",
    "\n",
    "\n",
    "ax4 = plt.gca()\n",
    "ax4.tick_params(axis = 'both', which = 'major', labelsize = fontsize)\n",
    "ax4.tick_params(axis = 'both', which = 'minor', labelsize = fontsize)\n",
    "\n",
    "\n",
    "plt.savefig(GIT_GRAPH_PATH + \"Degree_vs_Flow_errorbar.png\",\n",
    "            dpi=200,\n",
    "            format=\"PNG\",\n",
    "            transparent=False,\n",
    "            facecolor='white',\n",
    "            bbox_inches = \"tight\")\n",
    "\n",
    "\n",
    "\n",
    "if close_figures == True:\n",
    "    plt.close('all')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = 'Single'\n",
    "di = 'A'\n",
    "\n",
    "succ = Success_df.loc[(Success_df.Condition==con) & (Success_df['Direction']==di)].copy()\n",
    "succ = succ.drop([\"Start\", \"Target\",\"Condition\",\"Direction\",\"Leader\"], axis=1)\n",
    "succ = succ.dropna(axis=1).copy()\n",
    "y = succ.mean(axis=1).copy()\n",
    "\n",
    "yerr = succ.std(axis=1).copy()\n",
    "\n",
    "succ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow2succ = pd.DataFrame(columns=[\"Condition\", \"Direction\", \"Correlation\"])\n",
    "\n",
    "df_idx = 0\n",
    "\n",
    "for con in conditions:\n",
    "    for di in directions:\n",
    "        \n",
    "        flow_mean = Flow_df.loc[(Flow_df.Condition==con) & (Flow_df['Direction']==di)].dropna(axis=1).mean(axis=1).copy()\n",
    "        flow_std = Flow_df.loc[(Flow_df.Condition==con) & (Flow_df['Direction']==di)].dropna(axis=1).std(axis=1).copy()\n",
    "        \n",
    "        \n",
    "        succ = Success_df.loc[(Success_df.Condition==con) & (Success_df['Direction']==di)].copy()\n",
    "        succ = succ.drop([\"Start\", \"Target\",\"Condition\",\"Direction\",\"Leader\"], axis=1)\n",
    "        succ = succ.dropna(axis=1).copy()\n",
    "        succ_mean = succ.mean(axis=1).copy()\n",
    "\n",
    "        succ_std = succ.std(axis=1).copy()\n",
    "        \n",
    "        \n",
    "        flow2succ.loc[df_idx, \"Condition\"] = con\n",
    "        flow2succ.loc[df_idx, \"Direction\"] = di\n",
    "        flow2succ.loc[df_idx, \"Correlation\"] = np.corrcoef(flow_mean, succ_mean)[0,1]\n",
    "        \n",
    "        \n",
    "        df_idx +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow buildings to success analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo code:\n",
    "* for Condition in [SingleA, SingleB]\n",
    "    * for path in range(9)\n",
    "        * for subject in subIDs\n",
    "            * Compute flow graph\n",
    "            * add all houses of this path to a list \n",
    "            * compute average degree of those buildings for the subject \n",
    "            * Normalize with the total average degree of all houses of this subject\n",
    "            * Check and save whether subject was successful in this path \n",
    "\n",
    "Result: degree_dataframe (9 paths x 26 subjects), success_dataframe (9 paths x 26 subjects)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for optimization\n",
    "\n",
    "# Select the condition\n",
    "conditions = ['Single']\n",
    "directions = ['A','B']\n",
    "\n",
    "# Dictionary for all flow buildings on paths\n",
    "Flow_buildings_Single_dict = {}\n",
    "\n",
    "mean_house_degree = centrality_df.copy()\n",
    "mean_house_degree = mean_house_degree.drop(['Mean','STD'], axis=1)\n",
    "mean_house_degree = mean_house_degree.iloc[:-2,:].copy()\n",
    "\n",
    "mean_all = np.nanmean(np.array(mean_house_degree.iloc[:,1:]))\n",
    "std_all = np.nanstd(np.array(mean_house_degree.iloc[:,1:]))\n",
    "\n",
    "normalized_degree_subject_df = pd.DataFrame()\n",
    "target_norm_df = pd.DataFrame()\n",
    "degree_df = pd.DataFrame()\n",
    "start_normalized_df = pd.DataFrame()\n",
    "optim_normalized_df = pd.DataFrame()\n",
    "success_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    \n",
    "for condition in conditions:\n",
    "    \n",
    "    PROCESSED_DATA_PATH = './Results/' + condition + '/'\n",
    "    PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "\n",
    "\n",
    "    # open the subject info file of the condition\n",
    "    with open(PROCESSED_DATA_PATH + condition + '_Performance_Analysis.csv') as f:\n",
    "        try:\n",
    "            subject_data = pd.read_csv(f)\n",
    "        except:\n",
    "                print(\"\\tCould not load subject info - \" + str(condition) + \"!\")\n",
    "        \n",
    "        \n",
    "    subject_data_performance = subject_data[['SubjectID',\n",
    "                                         'Condition',\n",
    "                                         'P1:Success',\n",
    "                                         'P2:Success',\n",
    "                                         'P3:Success',\n",
    "                                         'P4:Success',\n",
    "                                         'P5:Success',\n",
    "                                         'P6:Success',\n",
    "                                         'P7:Success',\n",
    "                                         'P8:Success',\n",
    "                                         'P9:Success']].copy()\n",
    "\n",
    "    for idx in range(len(subject_data_performance)):\n",
    "        if subject_data_performance.loc[idx, 'Condition'] == 'B':\n",
    "            subject_data_performance.iloc[idx, 2:] = subject_data_performance.iloc[idx, 2:].values[::-1]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    for path in range(9):\n",
    "        print('Path ' + str(path+1) + ' started')\n",
    "\n",
    "        path_str = 'Path_' + str(path+1)\n",
    "        path_success_str = \"P{}:Success\".format(path+1)\n",
    "\n",
    "\n",
    "        subIDs = subject_data.SubjectID.values\n",
    "\n",
    "        subcount = 0\n",
    "\n",
    "        normalized_degrees_subject = []\n",
    "        target_normalized = []\n",
    "        start_normalized = []\n",
    "        optim_normalized = []\n",
    "        degrees = []\n",
    "        successes = []\n",
    "\n",
    "        # first run through subjects to create the relevant path buildings list\n",
    "        for subject in subIDs:\n",
    "            \n",
    "            # create the graph, nodelist and degree_dict\n",
    "            [G, nodelist, degree_dict] = create_graph(subject, GIT_GRAPH_PATH)\n",
    "\n",
    "            # Prepation  \n",
    "            sources = dests_A[:-1]\n",
    "            sinks = dests_A[1:]  \n",
    "\n",
    "                \n",
    "\n",
    "            flow_dict_out = dict()\n",
    "\n",
    "            source = sources[path]\n",
    "            sink = sinks[path]\n",
    "            \n",
    "            if subject_data_performance[subject_data_performance.SubjectID == subject]['Condition'].item() == 'A':\n",
    "                source_degree = mean_house_degree[mean_house_degree.Subject == str(subject)][source].item()\n",
    "                target_degree = mean_house_degree[mean_house_degree.Subject == str(subject)][sink].item()\n",
    "            else:\n",
    "                source_degree = mean_house_degree[mean_house_degree.Subject == str(subject)][sink].item()\n",
    "                target_degree = mean_house_degree[mean_house_degree.Subject == str(subject)][source].item()\n",
    "\n",
    "            flow_value, flow_dict = nx.maximum_flow(G, source, sink, capacity='capactiy')\n",
    "            \n",
    "            flow_buildings = []\n",
    "\n",
    "            for a,b in flow_dict.items():\n",
    "                temp = {x:y for x,y in b.items() if y!=0}\n",
    "\n",
    "                if temp!={}:\n",
    "                    flow_dict_out[a] = temp\n",
    "\n",
    "                    for i in temp.keys():\n",
    "                        flow_buildings.append(a)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "            # append average degrees\n",
    "            average_degree_all = []\n",
    "            average_degree_buildings = []\n",
    "            norm_degree_dist = []\n",
    "\n",
    "            # average degree over all subject buildings, NOT normalized\n",
    "            average_degree_all = \\\n",
    "                mean_house_degree[mean_house_degree.Subject==str(subject)].iloc[:,1:].mean(axis=1).item()\n",
    "            \n",
    "            # STD of degree over all subject buildings, NOT normalized\n",
    "            std_degree_all = \\\n",
    "                mean_house_degree[mean_house_degree.Subject==str(subject)].iloc[:,1:].std(axis=1).item()\n",
    "            \n",
    "            # Degree list subject normalized\n",
    "            norm_degree_dist = (mean_house_degree[mean_house_degree.Subject==str(subject)].iloc[:,1:] - average_degree_all)/std_degree_all\n",
    "            \n",
    "            # Degree list of flow buildings of subject normalized\n",
    "            average_degree_buildings = \\\n",
    "                np.nanmean(mean_house_degree[mean_house_degree.Subject==str(subject)].iloc[:,1:][np.unique(flow_buildings)])   \n",
    "                \n",
    "            # Average Degree of Flow Buildings\n",
    "            average_degree_buildings_norm = \\\n",
    "                np.nanmean(norm_degree_dist[np.unique(flow_buildings)])\n",
    "\n",
    "            \n",
    "            # Average Degree of Flow Buildings Normalized\n",
    "            degrees.append(average_degree_buildings)\n",
    "            normalized_degrees_subject.append(average_degree_buildings_norm)\n",
    "        \n",
    "            \n",
    "            # Target_Degree_Integration \n",
    "            target_degree_norm = (target_degree-average_degree_all)/std_degree_all\n",
    "            target_normalized.append(target_degree_norm)\n",
    "\n",
    "            \n",
    "            # Start_Degree_Integration\n",
    "            start_degree_norm = (source_degree-average_degree_all)/std_degree_all\n",
    "            start_normalized.append(start_degree_norm)\n",
    "            \n",
    "            \n",
    "            # append successes\n",
    "            subject_success = subject_data_performance[subject_data_performance.SubjectID == subject][path_success_str].item()\n",
    "            successes.append(subject_success)\n",
    "\n",
    "\n",
    "        normalized_degree_subject_df[path_str] = normalized_degrees_subject\n",
    "        target_norm_df[path_str] = target_normalized\n",
    "        start_normalized_df[path_str] = start_normalized\n",
    "        degree_df[path_str] = degrees\n",
    "        success_df[path_str] = successes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combining into one dataset for optimization\n",
    "\n",
    "cols = ['Path_' + str(i) for i in range(1,10)]\n",
    "normalized_degree_subject_df = normalized_degree_subject_df.fillna(0)\n",
    "normalized_degree_subject_df_flattened = pd.lreshape(normalized_degree_subject_df, {'Vals':cols});\n",
    "\n",
    "target_norm_df = target_norm_df.fillna(0)\n",
    "target_norm_df_flattened = pd.lreshape(target_norm_df, {'Vals':cols});\n",
    "\n",
    "start_normalized_df = start_normalized_df.fillna(0)\n",
    "start_normalized_df_flattened = pd.lreshape(start_normalized_df, {'Vals':cols});\n",
    "\n",
    "successes_flattened = pd.lreshape(success_df, {'Vals':cols});\n",
    "\n",
    "optim_dataset = pd.DataFrame(index = range(len(target_norm_df_flattened)), columns=['Start','Flow','Target','Success'])\n",
    "optim_dataset['Start'] = start_normalized_df_flattened\n",
    "optim_dataset['Flow'] = normalized_degree_subject_df_flattened\n",
    "optim_dataset['Target'] = target_norm_df_flattened\n",
    "optim_dataset['Success'] = successes_flattened\n",
    "\n",
    "optim_dataset\n",
    "\n",
    "m.dot([0.33,0.33,0.33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, LinearConstraint\n",
    "# Optimizing weight of start house, flow houses, target house in convex combination to predict succ/fail\n",
    "\n",
    "def optim_targ(inp_x, dataset):\n",
    "    inputs = np.array(dataset.drop(['Success'], axis=1));\n",
    "    inputs_combined = inputs.dot(inp_x).reshape(-1,1)\n",
    "    target = np.array(dataset.Success)\n",
    "    \n",
    "    clf = LogisticRegression().fit(inputs_combined, target);\n",
    "    return -clf.score(inputs_combined,target)\n",
    "            \n",
    "# Setting the constraints (all have to be positive, all have to add up to 1)\n",
    "param_const_1 = LinearConstraint([1,1,1],1,1)\n",
    "param_const_a = LinearConstraint([1,0,0],0, np.inf)\n",
    "param_const_b = LinearConstraint([0,1,0],0, np.inf)\n",
    "param_const_c = LinearConstraint([0,0,1],0, np.inf)\n",
    "consts = [param_const_1, param_const_a, param_const_b, param_const_c];\n",
    "\n",
    "# Trying different solving methods\n",
    "for meth in ['Nelder-Mead','Powell','CG','BFGS','L-BFGS-B','TNC'\\\n",
    "            ,'SLSQP']:\n",
    "    print(meth);\n",
    "    try:\n",
    "        res = minimize(optim_targ, np.array([0.0, 0.0, 0.0]), method=meth, args=(optim_dataset),constraints=consts)\n",
    "        print(res.fun, res.x)\n",
    "    except:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual optimization (don't tell anyone I did this)\n",
    "\n",
    "# Create grid of possible proportions that add up to 1\n",
    "m = np.linspace(0,1,100);\n",
    "arr = []\n",
    "for a in m:\n",
    "    for b in m:\n",
    "        for c in m:\n",
    "            \n",
    "            if(a+b+c > 0.98 and a+b+c < 1.02): arr.append([a,b,c])\n",
    "\n",
    "grid = np.array(arr)\n",
    "\n",
    "# Run logistic regression for all proportions in grid\n",
    "max_score = 0\n",
    "max_coef = 0\n",
    "max_int = 0\n",
    "max_prop = 0\n",
    "for prop in grid:\n",
    "    inputs = np.array(optim_dataset.drop(['Success'], axis=1));\n",
    "    inputs_combined = inputs.dot(prop).reshape(-1,1)\n",
    "    target = np.array(optim_dataset.Success)\n",
    "\n",
    "    clf = LogisticRegression().fit(inputs_combined, target);\n",
    "    sc = clf.score(inputs_combined,target)\n",
    "    if(sc > max_score):\n",
    "        max_score = sc;\n",
    "        max_coef = clf.coef_;\n",
    "        max_int = clf.intercept_;\n",
    "        max_prop = prop;\n",
    "\n",
    "# Plot the best proportions\n",
    "max_inputs_combined = inputs.dot(max_prop).reshape(-1,1)\n",
    "x = np.linspace(min(max_inputs_combined),max(max_inputs_combined),100)\n",
    "print(max_prop)\n",
    "plt.scatter(max_inputs_combined,target)\n",
    "plt.plot(x, sigmoid(x * max_coef[0] + max_int))\n",
    "plt.title('Score: ' + str(max_score))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saving_wanted = False\n",
    "plotting_wanted = False\n",
    "\n",
    "# Select the condition\n",
    "conditions = ['Single']\n",
    "directions = ['A','B']\n",
    "\n",
    "# Dictionary for all flow buildings on paths\n",
    "Flow_buildings_Single_dict = {}\n",
    "\n",
    "mean_house_degree = centrality_df.copy()\n",
    "mean_house_degree = mean_house_degree.drop(['Mean','STD'], axis=1)\n",
    "mean_house_degree = mean_house_degree.iloc[:-2,:].copy()\n",
    "\n",
    "mean_all = np.nanmean(np.array(mean_house_degree.iloc[:,1:]))\n",
    "std_all = np.nanstd(np.array(mean_house_degree.iloc[:,1:]))\n",
    "\n",
    "normalized_degree_subject_df = pd.DataFrame()\n",
    "target_norm_df = pd.DataFrame()\n",
    "degree_df = pd.DataFrame()\n",
    "start_normalized_df = pd.DataFrame()\n",
    "optim_normalized_df = pd.DataFrame()\n",
    "success_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    \n",
    "for condition in conditions:\n",
    "    \n",
    "    PROCESSED_DATA_PATH = './Results/' + condition + '/'\n",
    "    PROCESSED_DATA_FOLDER = sorted([f for f in os.listdir(PROCESSED_DATA_PATH) if not f.startswith('.')], key=str.lower)\n",
    "\n",
    "\n",
    "    # open the subject info file of the condition\n",
    "    with open(PROCESSED_DATA_PATH + condition + '_Performance_Analysis.csv') as f:\n",
    "        try:\n",
    "            subject_data = pd.read_csv(f)\n",
    "        except:\n",
    "                print(\"\\tCould not load subject info - \" + str(condition) + \"!\")\n",
    "        \n",
    "        \n",
    "    subject_data_performance = subject_data[['SubjectID',\n",
    "                                         'Condition',\n",
    "                                         'P1:Success',\n",
    "                                         'P2:Success',\n",
    "                                         'P3:Success',\n",
    "                                         'P4:Success',\n",
    "                                         'P5:Success',\n",
    "                                         'P6:Success',\n",
    "                                         'P7:Success',\n",
    "                                         'P8:Success',\n",
    "                                         'P9:Success']].copy()\n",
    "\n",
    "    for idx in range(len(subject_data_performance)):\n",
    "        if subject_data_performance.loc[idx, 'Condition'] == 'B':\n",
    "            subject_data_performance.iloc[idx, 2:] = subject_data_performance.iloc[idx, 2:].values[::-1]\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    for path in range(9):\n",
    "        print('Path ' + str(path+1) + ' started')\n",
    "\n",
    "        path_str = 'Path_' + str(path+1)\n",
    "        path_success_str = \"P{}:Success\".format(path+1)\n",
    "\n",
    "\n",
    "        subIDs = subject_data.SubjectID.values\n",
    "\n",
    "        subcount = 0\n",
    "\n",
    "        normalized_degrees_subject = []\n",
    "        target_normalized = []\n",
    "        start_normalized = []\n",
    "        optim_normalized = []\n",
    "        degrees = []\n",
    "        successes = []\n",
    "\n",
    "        # first run through subjects to create the relevant path buildings list\n",
    "        for subject in subIDs:\n",
    "            \n",
    "            # create the graph, nodelist and degree_dict\n",
    "            [G, nodelist, degree_dict] = create_graph(subject, GIT_GRAPH_PATH)\n",
    "\n",
    "            # Prepation  \n",
    "            sources = dests_A[:-1]\n",
    "            sinks = dests_A[1:]  \n",
    "\n",
    "                \n",
    "\n",
    "            flow_dict_out = dict()\n",
    "\n",
    "            source = sources[path]\n",
    "            sink = sinks[path]\n",
    "            \n",
    "            if subject_data_performance[subject_data_performance.SubjectID == subject]['Condition'].item() == 'A':\n",
    "                source_degree = mean_house_degree[mean_house_degree.Subject == str(subject)][source].item()\n",
    "                target_degree = mean_house_degree[mean_house_degree.Subject == str(subject)][sink].item()\n",
    "            else:\n",
    "                source_degree = mean_house_degree[mean_house_degree.Subject == str(subject)][sink].item()\n",
    "                target_degree = mean_house_degree[mean_house_degree.Subject == str(subject)][source].item()\n",
    "\n",
    "            flow_value, flow_dict = nx.maximum_flow(G, source, sink, capacity='capactiy')\n",
    "            \n",
    "            flow_buildings = []\n",
    "\n",
    "            for a,b in flow_dict.items():\n",
    "                temp = {x:y for x,y in b.items() if y!=0}\n",
    "\n",
    "                if temp!={}:\n",
    "                    flow_dict_out[a] = temp\n",
    "\n",
    "                    for i in temp.keys():\n",
    "                        flow_buildings.append(a)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                \n",
    "            # append average degrees\n",
    "            average_degree_all = []\n",
    "            average_degree_buildings = []\n",
    "            norm_degree_dist = []\n",
    "\n",
    "            # average degree over all subject buildings, NOT normalized\n",
    "            average_degree_all = \\\n",
    "                mean_house_degree[mean_house_degree.Subject==str(subject)].iloc[:,1:].mean(axis=1).item()\n",
    "            \n",
    "            # STD of degree over all subject buildings, NOT normalized\n",
    "            std_degree_all = \\\n",
    "                mean_house_degree[mean_house_degree.Subject==str(subject)].iloc[:,1:].std(axis=1).item()\n",
    "            \n",
    "            # Degree list subject normalized\n",
    "            norm_degree_dist = (mean_house_degree[mean_house_degree.Subject==str(subject)].iloc[:,1:] - average_degree_all)/std_degree_all\n",
    "            \n",
    "            # Degree list of flow buildings of subject normalized\n",
    "            average_degree_buildings = \\\n",
    "                np.nanmean(mean_house_degree[mean_house_degree.Subject==str(subject)].iloc[:,1:][np.unique(flow_buildings)])   \n",
    "                \n",
    "            # Average Degree of Flow Buildings\n",
    "            average_degree_buildings_norm = \\\n",
    "                np.nanmean(norm_degree_dist[np.unique(flow_buildings)])\n",
    "\n",
    "            \n",
    "            # Average Degree of Flow Buildings Normalized\n",
    "            degrees.append(average_degree_buildings)\n",
    "            normalized_degrees_subject.append(average_degree_buildings_norm)\n",
    "        \n",
    "            \n",
    "            # Target_Degree_Integration \n",
    "            target_degree_mean = (average_degree_buildings_norm + (target_degree-average_degree_all)/std_degree_all)/2\n",
    "            target_normalized.append(target_degree_mean)\n",
    "\n",
    "            \n",
    "            # Start_Degree_Integration\n",
    "            start_degree_mean = (average_degree_buildings_norm + (source_degree-average_degree_all)/std_degree_all)/2\n",
    "            start_normalized.append(start_degree_mean)\n",
    "            \n",
    "            \n",
    "            # append successes\n",
    "            subject_success = subject_data_performance[subject_data_performance.SubjectID == subject][path_success_str].item()\n",
    "            successes.append(subject_success)\n",
    "\n",
    "\n",
    "        normalized_degree_subject_df[path_str] = normalized_degrees_subject\n",
    "        target_norm_df[path_str] = target_normalized\n",
    "        start_normalized_df[path_str] = start_normalized\n",
    "        degree_df[path_str] = degrees\n",
    "        success_df[path_str] = successes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "dataframes = [degree_df, normalized_degree_subject_df, start_normalized_df, target_norm_df]\n",
    "dataframe_types = ['Average Degree of Flow Buildings', \n",
    "                   'Average Degree of Flow Buildings Normalized',\n",
    "                   'Start_Degree_Integration',\n",
    "                   'Target_Degree_Integration']\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for dataframe in dataframes:\n",
    "    \n",
    "    dataset = []\n",
    "    dataset = dataframe.copy()\n",
    "    dataset = np.array(dataset)\n",
    "    dataset = np.nan_to_num(dataset, copy=True, nan=1.0, posinf=None, neginf=None)\n",
    "    dataset = dataset.reshape(-1,1)\n",
    "    dataset = dataset.astype(float)\n",
    "\n",
    "    successes = []\n",
    "    successes = success_df.copy()\n",
    "    successes = np.array(successes)\n",
    "    successes = np.nan_to_num(successes, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "    successes = successes.reshape(-1,1)\n",
    "    successes = successes.astype(int)\n",
    "\n",
    "\n",
    "    plt.subplot(2,2,idx+1)\n",
    "    \n",
    "    clf = LogisticRegression().fit(dataset, successes)\n",
    "    coef = clf.coef_;\n",
    "    intercept = clf.intercept_;\n",
    "    x_in = np.linspace(min(dataset), max(dataset), 100);\n",
    "    y = sigmoid(coef * x_in + intercept)\n",
    "\n",
    "    plt.scatter(dataset, successes)\n",
    "    plt.plot(x_in, y);\n",
    "    fit_pctg = clf.score(dataset, successes);\n",
    "\n",
    "    plt.title(dataframe_types[idx] + ' - Score: ' + str(round(clf.score(dataset,successes),4)))\n",
    "    plt.xlabel(\"Normalized Degree\")\n",
    "    plt.ylabel(\"Success/Failure\")\n",
    "    \n",
    "    idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "\n",
    "#con_dir_degree = current_path_str + condition + '_' + direction + '_Degree'\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "    \n",
    "for path in range(9):\n",
    "\n",
    "    path_str = \"Path_\" + str(path+1)\n",
    "    dataset = []\n",
    "    dataset = target_norm_df[path_str].copy()\n",
    "    dataset = np.array(dataset)\n",
    "    dataset = np.nan_to_num(dataset, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "    dataset = dataset.reshape(-1,1)\n",
    "    dataset = dataset.astype(float)\n",
    "\n",
    "    successes = []\n",
    "    successes = success_df[path_str].copy()\n",
    "    successes = np.array(successes)\n",
    "    successes = np.nan_to_num(successes, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "    successes = successes.reshape(-1,1)\n",
    "    successes = successes.astype(int)\n",
    "\n",
    "    plt.subplot(3,3,path+1)\n",
    "    plt.title('Path ' + str(path))\n",
    "\n",
    "    clf = LogisticRegression().fit(dataset, successes)\n",
    "    coef = clf.coef_;\n",
    "    intercept = clf.intercept_;\n",
    "    x_in = np.linspace(min(dataset), max(dataset), 100);\n",
    "    y = sigmoid(coef * x_in + intercept)\n",
    "\n",
    "    plt.scatter(dataset, successes)\n",
    "    plt.plot(x_in, y);\n",
    "    fit_pctg = clf.score(dataset, successes);\n",
    "\n",
    "    plt.title('Path ' + str(path+1) + ' Score: ' + str(round(clf.score(dataset,successes),4)))\n",
    "    plt.xlabel(\"Degree proportion of path buildings / all buildings\")\n",
    "    plt.ylabel(\"Part of successful trial (yes/no)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
